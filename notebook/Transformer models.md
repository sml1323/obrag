---
tags:
  - transformer
  - llm-engineering
  - pipeline
  - week3
  - day4
  - ì–‘ìí™”
create: 2025-11-16 12:43:42
---
**Week3 Day4: Transformer Models** 

```ad-info
í•™ìŠµ ëª©í‘œ: HuggingFace Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ì˜ **ì €ìˆ˜ì¤€ API** í•™ìŠµ
```
- **Pipeline API**: `pipeline("text-generation")` - ì¶”ìƒí™”ëœ ê³ ìˆ˜ì¤€ ì¸í„°í˜ì´ìŠ¤
- **Model API**: `AutoModelForCausalLM.from_pretrained()` - PyTorch ê¸°ë°˜ ì €ìˆ˜ì¤€ ì œì–´

> [!focus]
> 1. ëª¨ë¸ ë‚´ë¶€ ë™ì‘ ì´í•´: Transformerê°€ ì–´ë–»ê²Œ ì‘ë™í•˜ëŠ”ì§€
> 2. ê³ ê¸‰ ì œì–´: ì–‘ìì™€(Quantization), í† í¬ë‚˜ì´ì§•, ìƒì„± íŒŒë¼ë¯¸í„° ì¡°ì •
> 3. ë©”ëª¨ë¦¬ ìµœì í™”: ì œí•œëœ GPU ë¦¬ì†ŒìŠ¤ì—ì„œ ëŒ€í˜• ëª¨ë¸ ì‹¤í–‰
> 4. ì‹¤ì „ ì¤€ë¹„: ì‹¤ì œ í”„ë¡œì íŠ¸ì—ì„œ ëª¨ë¸ ì»¤ìŠ¤í„°ë§ˆì´ì§• ëŠ¥ë ¥

```table-of-contents
```

**ëª¨ë¸ ë©”ëª¨ë¦¬ ì¤„ì¼ë•Œ ì°¸ê³ í•˜ê¸°**


# Section1: í™˜ê²½ ì„¤ì •
```python
!pip install -q --upgrade bitsandbytes accelerate
```
- `bitsandbytes`: ì–‘ìí™”(Quantization) ë¼ì´ë¸ŒëŸ¬ë¦¬
	- ëª¨ë¸ì„ 4bitë¡œ ì••ì¶•í•˜ì—¬ ë©”ëª¨ë¦¬ ì ˆì•½
	- 8Bëª¨ë¸ì„ 2GB ì •ë„ë¡œ ì¶•ì†Œ ê°€ëŠ¥
		- **Float32 (32-bit)**: ë†’ì€ ì •ë°€ë„, í° ë©”ëª¨ë¦¬
		- **BFloat16 (16-bit)**: ì¤‘ê°„ ì •ë°€ë„, ì ˆë°˜ ë©”ëª¨ë¦¬
		- **4-bit Quantization**: ë‚®ì€ ì •ë°€ë„, ìµœì†Œ ë©”ëª¨ë¦¬
		- **nf4 (NormalFloat4)**: ì •ê·œë¶„í¬ ìµœì í™” 4-bit í˜•ì‹
		- **Double Quantization**: ì–‘ìí™” íŒŒë¼ë¯¸í„°ë„ ì–‘ìí™” (ì¶”ê°€ ì••ì¶•)
- `accelerate`: ëª¨ë¸ì„ ì—¬ëŸ¬ GPU/CPUì— íš¨ìœ¨ì ìœ¼ë¡œ ë¶„ì‚°
	- `device_map=auto` ì œê³µ

```text
device_map="auto" â†’ ìë™ìœ¼ë¡œ ìµœì  ë°°ì¹˜
GPUì— ìµœëŒ€í•œ ì˜¬ë¦¬ê³ 
ë„˜ì¹˜ëŠ” ë¶€ë¶„ì€ CPU RAMìœ¼ë¡œ
ë””ìŠ¤í¬ê¹Œì§€ í™œìš© (ê·¹ë‹¨ì  ê²½ìš°)
```

## í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬
- `from huggingface_hub import login`: HuggingFace Hub ë¡œê·¸ì¸
- `from transformers import AutoTokenizer, AutoModelForCausalLm, TextStreamer, BitsAndBytesConfig`
	- `AutoTokenizer`: í…ìŠ¤íŠ¸ -> í† í° ID ë³€í™˜
	- `AutoModelForCausalLM`: ì–¸ì–´ ëª¨ë¸ ìë™ ë¡œë“œ
		- ëª¨ë¸ ì•„í‚¤í…ì²˜ë¥¼ ìë™ ê°ì§€í•˜ì—¬, í•˜ë‚˜ì˜ í´ë˜ìŠ¤ë¡œ ë‹¤ë¥¸ ì–¸ì–´ëª¨ë¸ í´ë˜ìŠ¤ ì‚¬ìš©í•  í•„ìš” ì—†ì´ ì‚¬ìš© ê°€ëŠ¥
	- `TextStreamer`: ìƒì„±ëœ í…ìŠ¤íŠ¸ ì‹¤ì‹œê°„ ì¶œë ¥(ë¶€ë“œëŸ½ê²Œ)
	- `BitsAndBytesConfig`: ì–‘ìí™” ì„¤ì • êµ¬ì„±
- `import torch`: ë”¥ëŸ¬ë‹ í”„ë ˆì„ì›Œí¬
- `import gc`: ê°€ë¹„ì§€ì½œë ‰ì…˜, ë©”ëª¨ë¦¬ ê´€ë¦¬ìš©

# Section2: HuggingFace ì¸ì¦ ë° ëª¨ë¸ ì—‘ì„¸ìŠ¤
## HuggingFace Hub
- **HuggingFace Hub**: ì˜¤í”ˆì†ŒìŠ¤ ëª¨ë¸ ì €ì¥ì†Œ (GitHubì˜ ëª¨ë¸ ë²„ì „)
- **API í† í°**: ê°œì¸ ì¸ì¦ í‚¤ (GitHub Personal Access Tokenê³¼ ìœ ì‚¬)
- **ê²Œì´íŠ¸ë“œ ëª¨ë¸ (Gated Models)**: Llamaì²˜ëŸ¼ ì•½ê´€ ë™ì˜ í›„ ì‚¬ìš© ê°€ëŠ¥í•œ ëª¨ë¸



```python
hf_token = userdata.get('HF_TOKEN')

login(hf_token, add_to_git_credential=True)
```
- hugging face ë¡œê·¸ì¸

# Section3: Quantization ì´í•´

## ëª¨ë¸ ì„ íƒ
```python
# see here: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct
LLAMA = "meta-llama/Meta-Llama-3.1-8B-Instruct"
# see here: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct
# LLAMA = "meta-llama/Llama-3.2-1B-Instruct"
PHI = "microsoft/Phi-4-mini-instruct"
GEMMA = "google/gemma-3-270m-it"
QWEN = "Qwen/Qwen3-4B-Instruct-2507"
DEEPSEEK = "deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B"
messages = [
{"role": "user", "content": "Tell a joke for a room of Data Scientists"}
]
```

```python
# Quantization Config - ì ì€ ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•´ì„œ ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì— loadí•˜ê²Œ í•´ì¤Œ
  quant_config = BitsAndBytesConfig(
      load_in_4bit=True,                      # 4-bit ì–‘ìí™” í™œì„±í™”
      bnb_4bit_use_double_quant=True,         # ì´ì¤‘ ì–‘ìí™”
      bnb_4bit_compute_dtype=torch.bfloat16,  # ê³„ì‚° íƒ€ì…
      bnb_4bit_quant_type="nf4"               # NormalFloat4
  )
```
````ad-note
Quantization íŒŒë¼ë¯¸í„° ì˜ë¯¸
  load_in_4bit=True
  - ì˜ë¯¸: 32-bit â†’ 4-bit (ë©”ëª¨ë¦¬ 1/8ë¡œ ì¶•ì†Œ)
  - íš¨ê³¼: 8B ëª¨ë¸ì´ 32GB â†’ 4GBë¡œ!
```Text
ë©”ëª¨ë¦¬(GB) = íŒŒë¼ë¯¸í„° ìˆ˜ Ã— ë¹„íŠ¸ ìˆ˜ Ã· 8 Ã· 1,000,000,000

ì˜ˆì‹œ: Llama 1B (1,000,000,000 íŒŒë¼ë¯¸í„°)
- FP32: 1B Ã— 32 Ã· 8 Ã· 1e9 = 4GB
- FP16: 1B Ã— 16 Ã· 8 Ã· 1e9 = 2GB
- 4-bit: 1B Ã— 4 Ã· 8 Ã· 1e9 = 0.5GB
```


  bnb_4bit_use_double_quant=True
  - ì˜ë¯¸: ì–‘ìí™” íŒŒë¼ë¯¸í„°ë„ ì–‘ìí™” (ë©”íƒ€ ì••ì¶•)
  - íš¨ê³¼: ì¶”ê°€ë¡œ ~0.4GB ì ˆì•½

  bnb_4bit_compute_dtype=torch.bfloat16
  - ì˜ë¯¸: ì €ì¥ì€ 4-bit, ê³„ì‚°ì€ bfloat16
  - íš¨ê³¼: ì†ë„ â†‘, ì •í™•ë„ ê· í˜•

  bnb_4bit_quant_type="nf4"
  - ì˜ë¯¸: NormalFloat4 (ì •ê·œë¶„í¬ ìµœì í™”)
  - ëŒ€ì•ˆ: "fp4" (uniform distribution)
````

**Trade-off ê·¸ë˜í”„**:

```
ì •í™•ë„ â†‘
â”‚
â”‚ FP32 â—â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€  (ëŠë¦¼, ì •í™•)
â”‚        â•²
â”‚         â•² FP16 â—â”€â”€â”€â”€â”€â”€  (ë¹ ë¦„, ì•½ê°„ ë¶€ì •í™•)
â”‚              â•²
â”‚               â•² bfloat16 â—  (ë¹ ë¦„, ê· í˜•)
â”‚                    â•²
â”‚                     â•² 4-bit â—  (ë§¤ìš° ë¹ ë¦„, ë¶€ì •í™•)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â†’ ì†ë„ â†‘
```

**bfloat16ì˜ íŠ¹ì§•**:

```python
torch.float32  # 32-bit: ë©”ëª¨ë¦¬ â†‘â†‘, ì •í™•ë„ â†‘â†‘, ì†ë„ â†“â†“
torch.float16  # 16-bit: ë©”ëª¨ë¦¬ â†“, ì •í™•ë„ â†“, ì†ë„ â†‘
torch.bfloat16 # 16-bit: ë©”ëª¨ë¦¬ â†“, ì •í™•ë„ â—‹, ì†ë„ â†‘

# bfloat16 vs float16 ì°¨ì´:
# - ë‘˜ ë‹¤ 16-bit
# - bfloat16: ì§€ìˆ˜ë¶€ ë„“ìŒ â†’ í° ìˆ«ì í‘œí˜„ ì¢‹ìŒ â†’ LLMì— ìœ ë¦¬
# - float16: ì •ë°€ë„ ë†’ìŒ â†’ ì‘ì€ ìˆ«ì ì •í™• â†’ ì´ë¯¸ì§€ ì²˜ë¦¬ì— ìœ ë¦¬
```

# Section4: Tokenizer
`````ad-info
## Tokenizer?
```ad-missing
LLMì€ í…ìŠ¤íŠ¸ë¥¼ ì´í•´ ëª»í•¨ -> ì˜¤ì§ ìˆ«ìë§Œ ì²˜ë¦¬
```

```ad-success
Tokenizer = í…ìŠ¤íŠ¸ $\iff$ ìˆ«ì ë³€í™˜ê¸°
```

```text
# ì‚¬ëŒì´ ë³´ëŠ” ê²ƒ
text = "Hello, world!"

# LLMì´ ë³´ëŠ” ê²ƒ
tokens = [9906, 11, 1917, 0]  # ìˆ«ì ë°°ì—´

# Tokenizerì˜ ì—­í• 
tokenizer.encode("Hello, world!")  â†’ [9906, 11, 1917, 0]
tokenizer.decode([9906, 11, 1917, 0]) â†’ "Hello, world!"
```


`````

```python
# Tokenizer ë¡œë“œ
tokenizer = AutoTokenizer.from_pretrained(LLAMA)
tokenizer.pad_token = tokenizer.eos_token
inputs = tokenizer.apply_chat_template(messages, return_tensors="pt").to("cuda")

print("âœ… Tokenizer ë¡œë“œ ì™„ë£Œ")
print(f"ëª¨ë¸: {LLAMA}")
print(f"Vocab í¬ê¸°: {len(tokenizer)}") # Llama Tokenizerì˜ ì–´íœ˜ì˜ í¬ê¸°

# í…ŒìŠ¤íŠ¸ ë¬¸ì¥
text = "ì•ˆë…•í•˜ì„¸ìš”! LLMì„ í•™ìŠµ ì¤‘ì…ë‹ˆë‹¤."

# ì¸ì½”ë”© (í…ìŠ¤íŠ¸ â†’ ìˆ«ì)
tokens = tokenizer.encode(text)
print(f"ì›ë³¸ í…ìŠ¤íŠ¸: {text}")
print(f"í† í° IDs: {tokens}")
print(f"í† í° ê°œìˆ˜: {len(tokens)}")

# ë””ì½”ë”© (ìˆ«ì â†’ í…ìŠ¤íŠ¸)
decoded = tokenizer.decode(tokens)
print(f"ë³µì›ëœ í…ìŠ¤íŠ¸: {decoded}")

# ê°œë³„ í† í° í™•ì¸
print("\nê°œë³„ í† í°:")
for i, token_id in enumerate(tokens):
    token_text = tokenizer.decode([token_id])
    print(f"  {i}: {token_id} â†’ '{token_text}'")
```

`tokenizer.pad_token = tokenizer.eos_token` **pad í† í°ì„ eos(endOfSentense) í† í°ê³¼ ë™ì¼ì‹œ í•˜ëŠ”ê²Œ ê´€ë¡€** 
```text
# ë¬¸ì œ: LlamaëŠ” ê¸°ë³¸ì ìœ¼ë¡œ pad_tokenì´ ì—†ìŒ
tokenizer.pad_token  # None

# í•´ê²°: EOS í† í°ì„ íŒ¨ë”©ìœ¼ë¡œ ì¬ì‚¬ìš©
tokenizer.pad_token = tokenizer.eos_token  # <|end_of_text|>

# ì™œ í•„ìš”í•œê°€?
# ë°°ì¹˜ ì²˜ë¦¬ ì‹œ ê¸¸ì´ë¥¼ ë§ì¶°ì•¼ í•¨
ë¬¸ì¥1: [1, 2, 3]           # 3 í† í°
ë¬¸ì¥2: [4, 5, 6, 7, 8]     # 5 í† í°

# íŒ¨ë”© í›„:
ë¬¸ì¥1: [1, 2, 3, PAD, PAD] # 5 í† í°
ë¬¸ì¥2: [4, 5, 6, 7, 8]     # 5 í† í°

```

`inputs = tokenizer.apply_chat_template(messages, return_tensors="pt").to("cuda")`
```text
# 1. ì±„íŒ… í˜•ì‹ìœ¼ë¡œ ë³€í™˜
messages = [
    {"role": "system", "content": "You are helpful"},
    {"role": "user", "content": "Tell me a joke"}
]

# 2. apply_chat_templateì´ ìë™ìœ¼ë¡œ:
text = """<|begin_of_text|><|start_header_id|>system<|end_header_id|>
You are helpful<|eot_id|>
<|start_header_id|>user<|end_header_id|>
Tell me a joke<|eot_id|>"""

# 3. í† í¬ë‚˜ì´ì§•
inputs = tokenizer.encode(text)

# 4. PyTorch í…ì„œë¡œ ë³€í™˜
return_tensors="pt"  â†’ torch.Tensor

# 5. GPUë¡œ ì´ë™
.to("cuda")
```

# Section 5: Model êµ¬ì¡° íƒìƒ‰

```python
model = AutoModelForCausalLM.from_pretrained(
    LLAMA, 
    device_map="auto", 
    quantization_config=quant_config
)

memory = model.get_memory_footprint() / 1e6
# Memory footprint: 5,591.5 MB
```
```text
# ëª¨ë¸ì´ GPU ë©”ëª¨ë¦¬ì—ì„œ ì°¨ì§€í•˜ëŠ” í¬ê¸°

# Llama 1B ëª¨ë¸:
# - ì›ë³¸ (FP32): ~4GB
# - 4-bit ì–‘ìí™”: ~1GB
# - ì‹¤ì œ ì¸¡ì •: 5,591 MB â‰ˆ 5.6GB

# ì™œ 1GBë³´ë‹¤ í°ê°€?
# - ëª¨ë¸ ê°€ì¤‘ì¹˜: ~1GB
# - Quantization ë©”íƒ€ë°ì´í„°: ~200MB
# - GPU ë²„í¼/ìºì‹œ: ~4GB (ë™ì  í• ë‹¹)
# - Special tokens embedding: ~100MB
```

`model`
```text
LlamaForCausalLM(
  (model): LlamaModel(
    (embed_tokens): Embedding(128256, 4096)
    (layers): ModuleList(
      (0-31): 32 x LlamaDecoderLayer(
        (self_attn): LlamaAttention(
          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)
          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)
          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)
        )
        (mlp): LlamaMLP(
          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)
          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)
          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)
          (act_fn): SiLUActivation()
        )
        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)
      )
    )
    (norm): LlamaRMSNorm((4096,), eps=1e-05)
    (rotary_emb): LlamaRotaryEmbedding()
  )
  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)
)
```

1. **Embedding**: ë‹¨ì–´ â†’ ë²¡í„° (128K ë‹¨ì–´ â†’ 4K ì°¨ì›)
2. **32 Layers**: ë°˜ë³µ í•™ìŠµ (ë” ë§ìœ¼ë©´ ë” ë˜‘ë˜‘)
3. **Attention**: ë¬¸ë§¥ ì´í•´ ("ì€í–‰" â†’ ê°•ê°€? ê¸ˆìœµ?)
4. **MLP**: íŒ¨í„´ ë³€í™˜
5. **lm_head**: ë‹¤ìŒ ë‹¨ì–´ ì˜ˆì¸¡ (4K â†’ 128K í™•ë¥ )
6. **4-bit**: ë©”ëª¨ë¦¬ 8ë°° ì ˆì•½


# Section 6: Text Generation
```python
outputs = model.generate(inputs, max_new_tokens=80)

outputs[0]
```
```python
The attention mask and the pad token id were not set. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
Setting `pad_token_id` to `eos_token_id`:128001 for open-end generation.
The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.
```
### A. ê²½ê³  ë©”ì‹œì§€ì˜ í•µì‹¬ (Attention Mask)
- **Attention Mask (ì–´í…ì…˜ ë§ˆìŠ¤í¬):**Â LLMì€ ì—¬ëŸ¬ ë¬¸ì¥(ë°°ì¹˜)ì„ í•œ ë²ˆì— ì²˜ë¦¬í•  ë•Œ, ë¬¸ì¥ì˜ ê¸¸ì´ê°€ ì œê°ê°ì´ë¯€ë¡œ ê°€ì¥ ê¸´ ë¬¸ì¥ì— ë§ì¶° **íŒ¨ë”©(Padding)**ì´ë¼ëŠ” íŠ¹ìˆ˜ í† í°ì„ ë’¤ì— ì±„ì›Œ ë„£ìŠµë‹ˆë‹¤.
    - **ë§ˆìŠ¤í¬ì˜ ì—­í• :**Â ì–´í…ì…˜ ë§ˆìŠ¤í¬ëŠ” ëª¨ë¸ì—ê²Œ "ì´ ë¶€ë¶„ì€ ì§„ì§œ ë°ì´í„°ê°€ ì•„ë‹ˆê³ Â **íŒ¨ë”©**ì´ë‹ˆê¹Œ ì–´í…ì…˜(ì§‘ì¤‘)ì„ í•  í•„ìš”ê°€ ì—†ë‹¤"ê³  ì•Œë ¤ì£¼ëŠ” ì—­í• ì„ í•©ë‹ˆë‹¤.
    - **ê²½ê³ ì˜ ì˜ë¯¸:**Â ì½”ë“œê°€ ì´ ë§ˆìŠ¤í¬ë¥¼ ëª…ì‹œì ìœ¼ë¡œ ì„¤ì •í•˜ì§€ ì•Šì•„ì„œ, ëª¨ë¸ì´ íŒ¨ë”© í† í°ì„ ì˜ëª» ì²˜ë¦¬í•˜ê³  ì›ì¹˜ ì•ŠëŠ” ê²°ê³¼ë¥¼ ìƒì„±í•  ìˆ˜ ìˆë‹¤ëŠ” ê²½ê³ ì…ë‹ˆë‹¤.
### B. ê²½ê³  ë©”ì‹œì§€ì˜ í•µì‹¬ (Pad Token ID)
- **Pad Token ID:**Â íŒ¨ë”© í† í°ì— í• ë‹¹ëœ ê³ ìœ  IDì…ë‹ˆë‹¤.
- **Setting pad_token_id to eos_token_id:128001...:**Â ì½”ë“œê°€Â pad_token_idë¥¼ ì„¤ì •í•˜ì§€ ì•Šì, ì‹œìŠ¤í…œì´ ìë™ìœ¼ë¡œÂ **ë¬¸ì¥ì˜ ë(End of Sentence,Â eos_token_id)**Â í† í° IDë¡œ ì„¤ì •í•´ ë²„ë ¸ë‹¤ëŠ” ì˜ë¯¸ì…ë‹ˆë‹¤.
    - **ë¬¸ì œì :**Â íŒ¨ë”© í† í°ê³¼ ë¬¸ì¥ ì¢…ë£Œ í† í°ì´ ê°™ì•„ì§€ë©´, ëª¨ë¸ì€ ë¬¸ì¥ì˜ ëìœ¼ë¡œ ì¸ì½”ë”©ëœ íŒ¨ë”© í† í°ì„ ë§Œë‚¬ì„ ë•Œ ìƒì„±ì„Â **ë©ˆì¶°ì•¼ í• ì§€**Â ì•„ë‹ˆë©´Â **ë¬´ì‹œí•˜ê³  ê³„ì†í•´ì•¼ í• ì§€**Â í˜¼ë€ì„ ê²ªì„ ìˆ˜ ìˆìŠµë‹ˆë‹¤. ì´ê²ƒì´ ë‘ ë²ˆì§¸ ê²½ê³ ì˜ ë‚´ìš©ì…ë‹ˆë‹¤.


```python
tokenizer.decode(outputs[0])
```
```python
<|begin_of_text|><|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|><|start_header_id|>user<|end_header_id|>\n\nTell a joke for a room of Data Scientists<|eot_id|><|start_header_id|>assistant<|end_header_id|>\n\nWhy did the regression model break up with the neural network?\n\nBecause it was a bad fit and the neural network was overfitting to the relationship.<|eot_id|>
```
#### 1. System Prompt (ì‹œìŠ¤í…œ í”„ë¡¬í”„íŠ¸)
```
<|start_header_id|>system<|end_header_id|>\n\nCutting Knowledge Date: December 2023\nToday Date: 26 Jul 2024\n\n<|eot_id|>
```
*   **ì—­í• :** ëª¨ë¸ì—ê²Œ í˜„ì¬ ìƒí™©ì— ëŒ€í•œ **ì»¨í…ìŠ¤íŠ¸(ë°°ê²½ ì •ë³´)**ë¥¼ ì œê³µí•©ë‹ˆë‹¤.
*   `Cutting Knowledge Date: December 2023`: ì´ ëª¨ë¸ì´ 2023ë…„ 12ì›”ê¹Œì§€ì˜ ë°ì´í„°ë¡œ í•™ìŠµë˜ì—ˆë‹¤ëŠ” **ì§€ì‹ì˜ ë§ˆê°ì¼**ì„ ì•Œë ¤ì¤ë‹ˆë‹¤.
*   `Today Date: 26 Jul 2024`: í˜„ì¬ ì‹œë®¬ë ˆì´ì…˜ ë‚ ì§œë¥¼ ì„¤ì •í•©ë‹ˆë‹¤.
#### 2. User Prompt (ì‚¬ìš©ì ì…ë ¥)

```
<|start_header_id|>user<|end_header_id|>\n\nTell a joke for a room of Data Scientists<|eot_id|>
```
*   **ì—­í• :** ì‚¬ìš©ì(User)ê°€ ëª¨ë¸ì—ê²Œ ì‹¤ì œë¡œ ìš”ì²­í•œ ëª…ë ¹ì–´ì…ë‹ˆë‹¤.
*   **ìš”ì²­:** "ë°ì´í„° ê³¼í•™ìë“¤ì„ ìœ„í•œ ë†ë‹´ì„ í•´ì¤˜."
#### 3. Assistant Response (ëª¨ë¸ ì‘ë‹µ)

```
<|start_header_id|>assistant<|end_header_id|>\n\nWhy did the regression model break up with the neural network?\n\nBecause it was a bad fit and the neural network was overfitting to the relationship.<|eot_id|>
```
*   **ì—­í• :** ëª¨ë¸(Assistant)ì´ ì‚¬ìš©ì ìš”ì²­ì— ëŒ€í•´ **ìƒì„±í•œ ì‘ë‹µ**ì…ë‹ˆë‹¤.
*   **ë‚´ìš© (ë°ì´í„° ê³¼í•™ ë†ë‹´):**
    *   **ì§ˆë¬¸:** ì™œ íšŒê·€ ëª¨ë¸(regression model)ì´ ì‹ ê²½ë§(neural network)ê³¼ í—¤ì–´ì¡Œì„ê¹Œìš”?
    *   **ë‹µë³€:** íšŒê·€ ëª¨ë¸ì€ **ì í•©ë„ê°€ ì•ˆ ì¢‹ì•˜ê³ (bad fit)**, ì‹ ê²½ë§ì€ ê´€ê³„ì— **ê³¼ì í•©(overfitting)**í–ˆê¸° ë•Œë¬¸ì…ë‹ˆë‹¤.
    *   *(ì°¸ê³ : `bad fit`ê³¼ `overfitting`ì€ ë°ì´í„° ê³¼í•™ì—ì„œ ëª¨ë¸ì˜ ì„±ëŠ¥ì„ í‰ê°€í•˜ëŠ” ì¤‘ìš”í•œ ìš©ì–´ì…ë‹ˆë‹¤.)*

# Section7: ì—¬ëŸ¬ ëª¨ë¸ ì‹¤í–‰
- ë©”ëª¨ë¦¬ ì •ë¦¬
```python
# Clean up memory
del model, inputs, tokenizer, outputs
gc.collect()
torch.cuda.empty_cache()
```
**ì‹¤í–‰í•˜ê³  ê²°ê³¼ í™•ì¸**:
```python
# GPU ë©”ëª¨ë¦¬ í™•ì¸
import torch
print(f"GPU í• ë‹¹: {torch.cuda.memory_allocated() / 1e9:.2f} GB")
print(f"GPU ì˜ˆì•½: {torch.cuda.memory_reserved() / 1e9:.2f} GB")
```

### Generate í•¨ìˆ˜
```python
def generate(model, messages, quant=True, max_new_tokens=80):
  tokenizer = AutoTokenizer.from_pretrained(model)
  tokenizer.pad_token = tokenizer.eos_token
  input_ids = tokenizer.apply_chat_template(messages, return_tensors="pt", add_generation_prompt=True).to("cuda")
  attention_mask = torch.ones_like(input_ids, dtype=torch.long, device="cuda")
  streamer = TextStreamer(tokenizer)
  
  if quant:
    model = AutoModelForCausalLM.from_pretrained(model, quantization_config=quant_config).to("cuda")
  else:
    model = AutoModelForCausalLM.from_pretrained(model).to("cuda")
    
  outputs = model.generate(input_ids=input_ids, attention_mask=attention_mask, max_new_tokens=max_new_tokens, streamer=streamer)
```

### ì—¬ëŸ¬ ëª¨ë¸ ì‹¤í–‰
```python
messages = [
    {"role": "user", "content": "Tell a joke for a room of Data Scientists"}
]

# 1. Phi-4 (Microsoft)
generate(PHI, messages)

# 2. Gemma 3 (Google) - ì–‘ìí™” ì—†ìŒ
generate(GEMMA, messages, quant=False)

# 3. Qwen 3 (Alibaba)
generate(QWEN, messages)

# 4. DeepSeek R1 (ì¶”ë¡  ëª¨ë¸) - ë” ê¸´ ë‹µë³€
generate(DEEPSEEK, messages, quant=False, max_new_tokens=500)
```

```mermaid
graph TD
    A[ì…ë ¥ í…ìŠ¤íŠ¸: ì•ˆë…•í•˜ì„¸ìš”] --> B[Tokenizer]
    B --> C[Token IDs: 128000, 101036, ...]
    C --> D[Embedding Layer]
    D --> E[128K vocab â†’ 4K ì°¨ì› ë²¡í„°]
    E --> F[Layer 1: Attention + MLP]
    F --> G[Layer 2: Attention + MLP]
    G --> H[...]
    H --> I[Layer 32: Attention + MLP]
    I --> J[Normalization]
    J --> K[LM Head]
    K --> L[128K í™•ë¥  ë¶„í¬]
    L --> M[ìµœê³  í™•ë¥  ì„ íƒ]
    M --> N[ì¶œë ¥ í† í°: ë°˜ê°€ì›Œìš”]
    N --> O[Tokenizer.decode]
    O --> P[ìµœì¢… í…ìŠ¤íŠ¸: ë°˜ê°€ì›Œìš”]
```



## ğŸ“š ê³µì‹ ë¬¸ì„œ
- [HuggingFace Transformers](https://huggingface.co/docs/transformers)
- [BitsAndBytes ë¬¸ì„œ](https://github.com/TimDettmers/bitsandbytes)
- [PyTorch ê³µì‹ ë¬¸ì„œ](https://pytorch.org/docs)

## ğŸ“ ì‹¬í™” í•™ìŠµ
- [Attention Is All You Need (ë…¼ë¬¸)](https://arxiv.org/abs/1706.03762)
- [Llama ì•„í‚¤í…ì²˜ ìƒì„¸](https://github.com/huggingface/transformers/blob/main/src/transformers/models/llama/modeling_llama.py)
- [Quantization ì‹¬í™”](https://arxiv.org/abs/2208.07339)

## ğŸ› ï¸ ì‹¤ì „ ì˜ˆì œ
- [HuggingFace Model Hub](https://huggingface.co/models)
- [Llama ì‚¬ìš© ì˜ˆì œ Colab](https://colab.research.google.com/drive/1deJO03YZTXUwcq2vzxWbiBhrRuI29Vo8)

## ğŸ’¬ ì»¤ë®¤ë‹ˆí‹°
- [HuggingFace Forums](https://discuss.huggingface.co/)
- [Reddit r/LocalLLaMA](https://www.reddit.com/r/LocalLLaMA/)

