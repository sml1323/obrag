{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c856ddb3",
   "metadata": {},
   "source": [
    "## Preprocessing before chuncking"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "34ddbdd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re \n",
    "\n",
    "from bisect import bisect_right\n",
    "\n",
    "import glob\n",
    "from typing import List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98d3359b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_knowledge_text(files) -> str:\n",
    "    entire_knowlegde_base = \"\"\n",
    "    with open(files, \"r\", encoding=\"utf-8\") as f:\n",
    "        entire_knowlegde_base += f.read()\n",
    "        entire_knowlegde_base += \"\\n\\n\"\n",
    "        \n",
    "    return entire_knowlegde_base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8612262",
   "metadata": {},
   "outputs": [],
   "source": [
    "base = \"Transformer models.md\"\n",
    "knowledge_text = make_knowledge_text(base)\n",
    "HEADER_RE = re.compile(r\"^(#{1,3})\\s+(.+?)\\s*$\",re.MULTILINE)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "1cb50e54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total character in knowledge_base: 13557\n"
     ]
    }
   ],
   "source": [
    "def return_knowledge_textlength(knowledge_base: str):\n",
    "    return len(knowledge_base)\n",
    "\n",
    "\n",
    "print(\n",
    "    f\"Total character in knowledge_base: {return_knowledge_textlength(knowledge_text)}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cbde5c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(592, ['Section1: í™˜ê²½ ì„¤ì •']),\n",
       " (1145, ['Section1: í™˜ê²½ ì„¤ì •', 'í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬']),\n",
       " (1584, ['Section2: HuggingFace ì¸ì¦ ë° ëª¨ë¸ ì—‘ì„¸ìŠ¤']),\n",
       " (1620, ['Section2: HuggingFace ì¸ì¦ ë° ëª¨ë¸ ì—‘ì„¸ìŠ¤', 'HuggingFace Hub']),\n",
       " (1921, ['Section3: Quantization ì´í•´']),\n",
       " (1950, ['Section3: Quantization ì´í•´', 'ëª¨ë¸ ì„ íƒ']),\n",
       " (1969,\n",
       "  ['see here: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct']),\n",
       " (2090, ['see here: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct']),\n",
       " (2158, ['LLAMA = \"meta-llama/Llama-3.2-1B-Instruct\"']),\n",
       " (2469, ['Quantization Config - ì ì€ ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•´ì„œ ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì— loadí•˜ê²Œ í•´ì¤Œ']),\n",
       " (3842, ['bfloat16 vs float16 ì°¨ì´:']),\n",
       " (3868, ['- ë‘˜ ë‹¤ 16-bit']),\n",
       " (3883, ['- bfloat16: ì§€ìˆ˜ë¶€ ë„“ìŒ â†’ í° ìˆ«ì í‘œí˜„ ì¢‹ìŒ â†’ LLMì— ìœ ë¦¬']),\n",
       " (3927, ['- float16: ì •ë°€ë„ ë†’ìŒ â†’ ì‘ì€ ìˆ«ì ì •í™• â†’ ì´ë¯¸ì§€ ì²˜ë¦¬ì— ìœ ë¦¬']),\n",
       " (3976, ['Section4: Tokenizer']),\n",
       " (4011, ['Section4: Tokenizer', 'Tokenizer?']),\n",
       " (4130, ['ì‚¬ëŒì´ ë³´ëŠ” ê²ƒ']),\n",
       " (4165, ['LLMì´ ë³´ëŠ” ê²ƒ']),\n",
       " (4216, ['Tokenizerì˜ ì—­í• ']),\n",
       " (4368, ['Tokenizer ë¡œë“œ']),\n",
       " (4669, ['í…ŒìŠ¤íŠ¸ ë¬¸ì¥']),\n",
       " (4709, ['ì¸ì½”ë”© (í…ìŠ¤íŠ¸ â†’ ìˆ«ì)']),\n",
       " (4842, ['ë””ì½”ë”© (ìˆ«ì â†’ í…ìŠ¤íŠ¸)']),\n",
       " (4924, ['ê°œë³„ í† í° í™•ì¸']),\n",
       " (5190, ['ë¬¸ì œ: LlamaëŠ” ê¸°ë³¸ì ìœ¼ë¡œ pad_tokenì´ ì—†ìŒ']),\n",
       " (5252, ['í•´ê²°: EOS í† í°ì„ íŒ¨ë”©ìœ¼ë¡œ ì¬ì‚¬ìš©']),\n",
       " (5337, ['ì™œ í•„ìš”í•œê°€?']),\n",
       " (5347, ['ë°°ì¹˜ ì²˜ë¦¬ ì‹œ ê¸¸ì´ë¥¼ ë§ì¶°ì•¼ í•¨']),\n",
       " (5432, ['íŒ¨ë”© í›„:']),\n",
       " (5601, ['1. ì±„íŒ… í˜•ì‹ìœ¼ë¡œ ë³€í™˜']),\n",
       " (5737, ['2. apply_chat_templateì´ ìë™ìœ¼ë¡œ:']),\n",
       " (5935, ['3. í† í¬ë‚˜ì´ì§•']),\n",
       " (5979, ['4. PyTorch í…ì„œë¡œ ë³€í™˜']),\n",
       " (6036, ['5. GPUë¡œ ì´ë™']),\n",
       " (6066, ['Section 5: Model êµ¬ì¡° íƒìƒ‰']),\n",
       " (6268, ['Memory footprint: 5,591.5 MB']),\n",
       " (6311, ['ëª¨ë¸ì´ GPU ë©”ëª¨ë¦¬ì—ì„œ ì°¨ì§€í•˜ëŠ” í¬ê¸°']),\n",
       " (6336, ['Llama 1B ëª¨ë¸:']),\n",
       " (6351, ['- ì›ë³¸ (FP32): ~4GB']),\n",
       " (6371, ['- 4-bit ì–‘ìí™”: ~1GB']),\n",
       " (6391, ['- ì‹¤ì œ ì¸¡ì •: 5,591 MB â‰ˆ 5.6GB']),\n",
       " (6420, ['ì™œ 1GBë³´ë‹¤ í°ê°€?']),\n",
       " (6434, ['- ëª¨ë¸ ê°€ì¤‘ì¹˜: ~1GB']),\n",
       " (6451, ['- Quantization ë©”íƒ€ë°ì´í„°: ~200MB']),\n",
       " (6482, ['- GPU ë²„í¼/ìºì‹œ: ~4GB (ë™ì  í• ë‹¹)']),\n",
       " (6510, ['- Special tokens embedding: ~100MB']),\n",
       " (7923, ['Section 6: Text Generation']),\n",
       " (8525, ['Section 6: Text Generation', 'A. ê²½ê³  ë©”ì‹œì§€ì˜ í•µì‹¬ (Attention Mask)']),\n",
       " (8872, ['Section 6: Text Generation', 'B. ê²½ê³  ë©”ì‹œì§€ì˜ í•µì‹¬ (Pad Token ID)']),\n",
       " (10884, ['Section7: ì—¬ëŸ¬ ëª¨ë¸ ì‹¤í–‰']),\n",
       " (10924, ['Clean up memory']),\n",
       " (11048, ['GPU ë©”ëª¨ë¦¬ í™•ì¸']),\n",
       " (11204, ['GPU ë©”ëª¨ë¦¬ í™•ì¸', 'Generate í•¨ìˆ˜']),\n",
       " (11947, ['GPU ë©”ëª¨ë¦¬ í™•ì¸', 'ì—¬ëŸ¬ ëª¨ë¸ ì‹¤í–‰']),\n",
       " (12063, ['1. Phi-4 (Microsoft)']),\n",
       " (12111, ['2. Gemma 3 (Google) - ì–‘ìí™” ì—†ìŒ']),\n",
       " (12182, ['3. Qwen 3 (Alibaba)']),\n",
       " (12230, ['4. DeepSeek R1 (ì¶”ë¡  ëª¨ë¸) - ë” ê¸´ ë‹µë³€']),\n",
       " (12811, ['4. DeepSeek R1 (ì¶”ë¡  ëª¨ë¸) - ë” ê¸´ ë‹µë³€', 'ğŸ“š ê³µì‹ ë¬¸ì„œ']),\n",
       " (13003, ['4. DeepSeek R1 (ì¶”ë¡  ëª¨ë¸) - ë” ê¸´ ë‹µë³€', 'ğŸ“ ì‹¬í™” í•™ìŠµ']),\n",
       " (13259, ['4. DeepSeek R1 (ì¶”ë¡  ëª¨ë¸) - ë” ê¸´ ë‹µë³€', 'ğŸ› ï¸ ì‹¤ì „ ì˜ˆì œ']),\n",
       " (13426, ['4. DeepSeek R1 (ì¶”ë¡  ëª¨ë¸) - ë” ê¸´ ë‹µë³€', 'ğŸ’¬ ì»¤ë®¤ë‹ˆí‹°'])]"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def extract_header_marks(text: str):\n",
    "    \"\"\"\n",
    "    ë¬¸ì„œë¥¼ ìœ„ì—ì„œ ì•„ë˜ë¡œ ì½ìœ¼ë©´ì„œ, \\n\n",
    "    â€œí˜„ì¬ ë‚´ê°€ ì†í•œ #/##/### ì„¹ì…˜ ê²½ë¡œâ€ë¥¼ ê³„ì† ì—…ë°ì´íŠ¸í•˜ê³ , \\n\n",
    "    í—¤ë”ê°€ ë“±ì¥í•˜ëŠ” ìœ„ì¹˜ë§ˆë‹¤ (ê·¸ ìœ„ì¹˜, ê·¸ë•Œì˜ ê²½ë¡œ)ë¥¼ ê¸°ë¡í•˜ëŠ” í•¨ìˆ˜\n",
    "    \"\"\"\n",
    "    header_marks = []\n",
    "    stack = [None, None, None]  # levels 1..3\n",
    "\n",
    "    for m in HEADER_RE.finditer(text):\n",
    "        level = len(m.group(1))\n",
    "        title = m.group(2).strip()\n",
    "\n",
    "        stack[level-1] = title\n",
    "        for i in range(level, 3): stack[i] = None\n",
    "        path = [x for x in stack if x]\n",
    "        header_marks.append((m.start(), path))\n",
    "    return header_marks\n",
    "extract_header_marks(knowledge_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e91ef2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(592, ['Section1: í™˜ê²½ ì„¤ì •']),\n",
       " (1145, ['Section1: í™˜ê²½ ì„¤ì •', 'í•„ìš” ë¼ì´ë¸ŒëŸ¬ë¦¬']),\n",
       " (1584, ['Section2: HuggingFace ì¸ì¦ ë° ëª¨ë¸ ì—‘ì„¸ìŠ¤']),\n",
       " (1620, ['Section2: HuggingFace ì¸ì¦ ë° ëª¨ë¸ ì—‘ì„¸ìŠ¤', 'HuggingFace Hub']),\n",
       " (1921, ['Section3: Quantization ì´í•´']),\n",
       " (1950, ['Section3: Quantization ì´í•´', 'ëª¨ë¸ ì„ íƒ']),\n",
       " (1969,\n",
       "  ['see here: https://huggingface.co/meta-llama/Meta-Llama-3.1-8B-Instruct']),\n",
       " (2090, ['see here: https://huggingface.co/meta-llama/Llama-3.2-1B-Instruct']),\n",
       " (2158, ['LLAMA = \"meta-llama/Llama-3.2-1B-Instruct\"']),\n",
       " (2469, ['Quantization Config - ì ì€ ë©”ëª¨ë¦¬ë¥¼ ì‚¬ìš©í•´ì„œ ëª¨ë¸ì„ ë©”ëª¨ë¦¬ì— loadí•˜ê²Œ í•´ì¤Œ']),\n",
       " (3842, ['bfloat16 vs float16 ì°¨ì´:']),\n",
       " (3868, ['- ë‘˜ ë‹¤ 16-bit']),\n",
       " (3883, ['- bfloat16: ì§€ìˆ˜ë¶€ ë„“ìŒ â†’ í° ìˆ«ì í‘œí˜„ ì¢‹ìŒ â†’ LLMì— ìœ ë¦¬']),\n",
       " (3927, ['- float16: ì •ë°€ë„ ë†’ìŒ â†’ ì‘ì€ ìˆ«ì ì •í™• â†’ ì´ë¯¸ì§€ ì²˜ë¦¬ì— ìœ ë¦¬']),\n",
       " (3976, ['Section4: Tokenizer']),\n",
       " (4011, ['Section4: Tokenizer', 'Tokenizer?']),\n",
       " (4130, ['ì‚¬ëŒì´ ë³´ëŠ” ê²ƒ']),\n",
       " (4165, ['LLMì´ ë³´ëŠ” ê²ƒ']),\n",
       " (4216, ['Tokenizerì˜ ì—­í• ']),\n",
       " (4368, ['Tokenizer ë¡œë“œ']),\n",
       " (4669, ['í…ŒìŠ¤íŠ¸ ë¬¸ì¥']),\n",
       " (4709, ['ì¸ì½”ë”© (í…ìŠ¤íŠ¸ â†’ ìˆ«ì)']),\n",
       " (4842, ['ë””ì½”ë”© (ìˆ«ì â†’ í…ìŠ¤íŠ¸)']),\n",
       " (4924, ['ê°œë³„ í† í° í™•ì¸']),\n",
       " (5190, ['ë¬¸ì œ: LlamaëŠ” ê¸°ë³¸ì ìœ¼ë¡œ pad_tokenì´ ì—†ìŒ']),\n",
       " (5252, ['í•´ê²°: EOS í† í°ì„ íŒ¨ë”©ìœ¼ë¡œ ì¬ì‚¬ìš©']),\n",
       " (5337, ['ì™œ í•„ìš”í•œê°€?']),\n",
       " (5347, ['ë°°ì¹˜ ì²˜ë¦¬ ì‹œ ê¸¸ì´ë¥¼ ë§ì¶°ì•¼ í•¨']),\n",
       " (5432, ['íŒ¨ë”© í›„:']),\n",
       " (5601, ['1. ì±„íŒ… í˜•ì‹ìœ¼ë¡œ ë³€í™˜']),\n",
       " (5737, ['2. apply_chat_templateì´ ìë™ìœ¼ë¡œ:']),\n",
       " (5935, ['3. í† í¬ë‚˜ì´ì§•']),\n",
       " (5979, ['4. PyTorch í…ì„œë¡œ ë³€í™˜']),\n",
       " (6036, ['5. GPUë¡œ ì´ë™']),\n",
       " (6066, ['Section 5: Model êµ¬ì¡° íƒìƒ‰']),\n",
       " (6268, ['Memory footprint: 5,591.5 MB']),\n",
       " (6311, ['ëª¨ë¸ì´ GPU ë©”ëª¨ë¦¬ì—ì„œ ì°¨ì§€í•˜ëŠ” í¬ê¸°']),\n",
       " (6336, ['Llama 1B ëª¨ë¸:']),\n",
       " (6351, ['- ì›ë³¸ (FP32): ~4GB']),\n",
       " (6371, ['- 4-bit ì–‘ìí™”: ~1GB']),\n",
       " (6391, ['- ì‹¤ì œ ì¸¡ì •: 5,591 MB â‰ˆ 5.6GB']),\n",
       " (6420, ['ì™œ 1GBë³´ë‹¤ í°ê°€?']),\n",
       " (6434, ['- ëª¨ë¸ ê°€ì¤‘ì¹˜: ~1GB']),\n",
       " (6451, ['- Quantization ë©”íƒ€ë°ì´í„°: ~200MB']),\n",
       " (6482, ['- GPU ë²„í¼/ìºì‹œ: ~4GB (ë™ì  í• ë‹¹)']),\n",
       " (6510, ['- Special tokens embedding: ~100MB']),\n",
       " (7923, ['Section 6: Text Generation']),\n",
       " (8525, ['Section 6: Text Generation', 'A. ê²½ê³  ë©”ì‹œì§€ì˜ í•µì‹¬ (Attention Mask)']),\n",
       " (8872, ['Section 6: Text Generation', 'B. ê²½ê³  ë©”ì‹œì§€ì˜ í•µì‹¬ (Pad Token ID)']),\n",
       " (10884, ['Section7: ì—¬ëŸ¬ ëª¨ë¸ ì‹¤í–‰']),\n",
       " (10924, ['Clean up memory']),\n",
       " (11048, ['GPU ë©”ëª¨ë¦¬ í™•ì¸']),\n",
       " (11204, ['GPU ë©”ëª¨ë¦¬ í™•ì¸', 'Generate í•¨ìˆ˜']),\n",
       " (11947, ['GPU ë©”ëª¨ë¦¬ í™•ì¸', 'ì—¬ëŸ¬ ëª¨ë¸ ì‹¤í–‰']),\n",
       " (12063, ['1. Phi-4 (Microsoft)']),\n",
       " (12111, ['2. Gemma 3 (Google) - ì–‘ìí™” ì—†ìŒ']),\n",
       " (12182, ['3. Qwen 3 (Alibaba)']),\n",
       " (12230, ['4. DeepSeek R1 (ì¶”ë¡  ëª¨ë¸) - ë” ê¸´ ë‹µë³€']),\n",
       " (12811, ['4. DeepSeek R1 (ì¶”ë¡  ëª¨ë¸) - ë” ê¸´ ë‹µë³€', 'ğŸ“š ê³µì‹ ë¬¸ì„œ']),\n",
       " (13003, ['4. DeepSeek R1 (ì¶”ë¡  ëª¨ë¸) - ë” ê¸´ ë‹µë³€', 'ğŸ“ ì‹¬í™” í•™ìŠµ']),\n",
       " (13259, ['4. DeepSeek R1 (ì¶”ë¡  ëª¨ë¸) - ë” ê¸´ ë‹µë³€', 'ğŸ› ï¸ ì‹¤ì „ ì˜ˆì œ']),\n",
       " (13426, ['4. DeepSeek R1 (ì¶”ë¡  ëª¨ë¸) - ë” ê¸´ ë‹µë³€', 'ğŸ’¬ ì»¤ë®¤ë‹ˆí‹°'])]"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def header_path_for_pos(header_marks, pos: int):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        - header_marks: list[(mark_pos, path)] sorted by mark_pos\n",
    "        - pos: chunk start position \n",
    "        \n",
    "    return: path that is in effect at pos (last mark_pos <= pos)\n",
    "    \"\"\"\n",
    "    \n",
    "    if not header_marks:\n",
    "        return []\n",
    "\n",
    "    positions = [p for p, _ in header_marks]\n",
    "    ins = bisect_right(positions, pos)   # insertion point to the right\n",
    "    idx = ins - 1\n",
    "\n",
    "    if idx < 0:\n",
    "        return []  # pos is before the first header\n",
    "    return header_marks[idx][1]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "dff00b2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def attach_header_metadata_to_chunks(doc_text: str, chunks):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        - doc_text: splitterì— ë„£ì€ 'ê·¸ í…ìŠ¤íŠ¸'ì™€ ë™ì¼í•œ ì›ë¬¸ ë¬¸ìì—´\n",
    "        - chunks: splitter ê²°ê³¼ (ch.page_content, ch.metadata ì¡´ì¬í•œë‹¤ê³  ê°€ì •)\n",
    "    \"\"\"\n",
    "    header_marks = extract_header_marks(doc_text)\n",
    "\n",
    "    search_from = 0\n",
    "    for ch in chunks:\n",
    "        snippet = ch.page_content\n",
    "\n",
    "        # 1) chunk í…ìŠ¤íŠ¸ê°€ ì›ë¬¸ì—ì„œ ì‹œì‘í•˜ëŠ” ìœ„ì¹˜ ì°¾ê¸°\n",
    "        pos = doc_text.find(snippet, search_from)\n",
    "        if pos == -1:\n",
    "            # ì•ˆì „ì¥ì¹˜: ëª» ì°¾ìœ¼ë©´ í—¤ë”ë¥¼ ë¹„ì›Œë‘ê±°ë‚˜ fallback\n",
    "            ch.metadata[\"header_path\"] = []\n",
    "            ch.metadata[\"section\"] = \"\"\n",
    "            continue\n",
    "\n",
    "        # 2) ë‹¤ìŒ chunkëŠ” ì´ë²ˆ chunk ë’¤ì—ì„œë¶€í„° ì°¾ê²Œ ì—…ë°ì´íŠ¸\n",
    "        search_from = pos + max(1, len(snippet) // 4)  # ë„ˆë¬´ ê³µê²©ì ì´ì§€ ì•Šê²Œ\n",
    "\n",
    "        # 3) í•´ë‹¹ ìœ„ì¹˜ì˜ header_path ì°¾ê¸°\n",
    "        hp = header_path_for_pos(header_marks, pos)\n",
    "\n",
    "        # 4) metadata ì£¼ì…\n",
    "        ch.metadata[\"header_path\"] = hp\n",
    "        ch.metadata[\"section\"] = \" > \".join(hp) if hp else \"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "ebb134d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "attach_header_metadata_to_chunks(knowledge_text, [])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f03c733",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "obrag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
