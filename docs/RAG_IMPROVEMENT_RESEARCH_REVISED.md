# Obsidian RAG ì‹œìŠ¤í…œ ê°œì„  ì—°êµ¬ ë³´ê³ ì„œ

> ì‘ì„±ì¼: 2026-02-05
> ì°¸ì¡° ë¦¬í¬ì§€í† ë¦¬: ApeRAG, agentic-rag-for-dummies, Kotaemon

---

## ëª©ì°¨

1. [í˜„ì¬ ì‹œìŠ¤í…œ ë¶„ì„](#1-í˜„ì¬-ì‹œìŠ¤í…œ-ë¶„ì„)
2. [í•œì˜ í˜¼ìš© ì„ë² ë”© ë¬¸ì œ í•´ê²°](#2-í•œì˜-í˜¼ìš©-ì„ë² ë”©-ë¬¸ì œ-í•´ê²°)
3. [GraphRAG ë° ì§€ì‹ ê·¸ë˜í”„](#3-graphrag-ë°-ì§€ì‹-ê·¸ë˜í”„)
4. [Agentic RAG íŒ¨í„´](#4-agentic-rag-íŒ¨í„´)
5. [Advanced RAG ê¸°ë²•](#5-advanced-rag-ê¸°ë²•)
6. [êµ¬í˜„ ìš°ì„ ìˆœìœ„ ë° ë¡œë“œë§µ](#6-êµ¬í˜„-ìš°ì„ ìˆœìœ„-ë°-ë¡œë“œë§µ)

---

## 1. í˜„ì¬ ì‹œìŠ¤í…œ ë¶„ì„

### 1.1 ì•„í‚¤í…ì²˜ ê°œìš”

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                     Current Architecture                      â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  Markdown Files                                               â”‚
â”‚       â”‚                                                       â”‚
â”‚       â–¼                                                       â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚FolderScannerâ”‚â”€â”€â”€â–¶â”‚MarkdownPrep  â”‚â”€â”€â”€â–¶â”‚ semantic_chunk â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                â”‚              â”‚
â”‚                                                â–¼              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚ChromaStore  â”‚â—€â”€â”€â”€â”‚EmbeddingStratâ”‚â—€â”€â”€â”€â”‚   Chunks       â”‚  â”‚
â”‚  â”‚(Vector DB)  â”‚    â”‚(OpenAI/BGE)  â”‚    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                         â”‚
â”‚         â”‚                                                     â”‚
â”‚         â–¼                                                     â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”  â”‚
â”‚  â”‚  Retriever  â”‚â”€â”€â”€â–¶â”‚PromptBuilder â”‚â”€â”€â”€â–¶â”‚   LLMStrategy  â”‚  â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜  â”‚
â”‚                                                â”‚              â”‚
â”‚                                                â–¼              â”‚
â”‚                                          RAGResponse          â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 í˜„ì¬ êµ¬ì„±ìš”ì†Œ

| êµ¬ì„±ìš”ì†Œ | í˜„ì¬ êµ¬í˜„ | íŒŒì¼ |
|---------|----------|------|
| **ì„ë² ë”©** | OpenAIEmbedder (default), SentenceTransformerEmbedder (BGE-m3) | `core/embedding/` |
| **ë²¡í„° DB** | ChromaDB (PersistentClient) | `db/chroma_store.py` |
| **ì²­í‚¹** | Header-based semantic chunking (min:200, max:1500) | `core/preprocessing/` |
| **ê²€ìƒ‰** | Dense vector search (L2 distance) | `core/rag/retriever.py` |
| **LLM** | OpenAI, Gemini, Ollama | `core/llm/` |

### 1.3 í˜„ì¬ ì‹œìŠ¤í…œì˜ í•œê³„

| í•œê³„ì  | ì„¤ëª… | ì˜í–¥ë„ |
|--------|------|--------|
| **ë‹¨ì¼ ì–¸ì–´ ì„ë² ë”©** | í•œê¸€/ì˜ì–´ í˜¼ìš© ì‹œ cross-lingual ê²€ìƒ‰ ì„±ëŠ¥ ì €í•˜ | ğŸ”´ High |
| **Linear RAG** | ë‹¨ìˆœ ê²€ìƒ‰â†’ìƒì„± íŒŒì´í”„ë¼ì¸, self-correction ì—†ìŒ | ğŸŸ¡ Medium |
| **No Graph** | ë¬¸ì„œ ê°„ ê´€ê³„/ì—”í‹°í‹° ì—°ê²° ì—†ìŒ | ğŸŸ¡ Medium |
| **No Reranking** | ê²€ìƒ‰ ê²°ê³¼ ì¬ì •ë ¬ ì—†ìŒ | ğŸŸ¡ Medium |
| **No Hybrid Search** | Dense searchë§Œ ì‚¬ìš©, keyword search ì—†ìŒ | ğŸŸ¡ Medium |

---

## 2. í•œì˜ í˜¼ìš© ì„ë² ë”© ë¬¸ì œ í•´ê²°

### 2.1 ë¬¸ì œ ì •ì˜

```
í˜„ì¬ ë¬¸ì œ:
- í•œê¸€ë¡œ ì‘ì„±ëœ ë…¸íŠ¸ â†’ ì˜ì–´ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰ ì‹œ ìœ ì‚¬ë„ ë‚®ìŒ
- ì˜ì–´ë¡œ ì‘ì„±ëœ ë…¸íŠ¸ â†’ í•œê¸€ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰ ì‹œ ìœ ì‚¬ë„ ë‚®ìŒ
- í•œì˜ í˜¼ìš© ë¬¸ì¥ì˜ ì¼ê´€ì„± ì—†ëŠ” ì„ë² ë”©
```

### 2.2 í•´ê²°ì±…: Multilingual Embedding Models

#### ê¶Œì¥ ëª¨ë¸ ë¹„êµ

| ëª¨ë¸ | ì°¨ì› | í•œêµ­ì–´ ì„±ëŠ¥ | íŠ¹ì§• | ì¶”ì²œë„ |
|------|------|------------|------|--------|
| **multilingual-e5-large-instruct** | 1024 | â­â­â­â­â­ | SOTA, instruction-tuned | ğŸ¥‡ 1ìˆœìœ„ |
| **BAAI/bge-m3** | 1024 | â­â­â­â­ | Dense+Sparse+ColBERT | ğŸ¥ˆ 2ìˆœìœ„ |
| **dragonkue/BGE-m3-ko** | 1024 | â­â­â­â­â­ | í•œêµ­ì–´ íŠ¹í™” fine-tuning | ğŸ¥‰ 3ìˆœìœ„ |
| **Alibaba-NLP/gte-multilingual-base** | 768 | â­â­â­â­ | 8192 í† í° ì»¨í…ìŠ¤íŠ¸ | ëŒ€ì•ˆ |

#### êµ¬í˜„ ì˜ˆì‹œ: Multilingual E5

```python
# src/core/embedding/multilingual_embedder.py

from sentence_transformers import SentenceTransformer
from typing import List
from .strategy import EmbeddingStrategy, Vector


class MultilingualE5Embedder(EmbeddingStrategy):
    """
    Microsoft Multilingual E5 ì„ë² ë”.
    í•œêµ­ì–´-ì˜ì–´ cross-lingual retrievalì— ìµœì í™”.
    """
    
    MODEL_NAME = "intfloat/multilingual-e5-large-instruct"
    
    def __init__(self, model_name: str = MODEL_NAME):
        self._model_name = model_name
        self._model = None
        self._dimension = 1024
    
    def _load_model(self):
        if self._model is None:
            self._model = SentenceTransformer(self._model_name)
    
    def embed(self, texts: List[str], is_query: bool = False) -> List[Vector]:
        """
        í…ìŠ¤íŠ¸ ì„ë² ë”© ìƒì„±.
        
        Args:
            texts: ì„ë² ë”©í•  í…ìŠ¤íŠ¸ ë¦¬ìŠ¤íŠ¸
            is_query: Trueë©´ ì¿¼ë¦¬ìš© prefix ì¶”ê°€
        """
        self._load_model()
        
        # E5 ëª¨ë¸ì€ prefixê°€ ì¤‘ìš”í•¨
        if is_query:
            texts = [f"query: {t}" for t in texts]
        else:
            texts = [f"passage: {t}" for t in texts]
        
        embeddings = self._model.encode(texts, convert_to_numpy=True)
        return embeddings.tolist()
    
    def embed_query(self, query: str) -> Vector:
        """ì¿¼ë¦¬ ì „ìš© ì„ë² ë”© (prefix ìë™ ì¶”ê°€)"""
        return self.embed([query], is_query=True)[0]
    
    def embed_documents(self, documents: List[str]) -> List[Vector]:
        """ë¬¸ì„œ ì „ìš© ì„ë² ë”© (passage prefix ì¶”ê°€)"""
        return self.embed(documents, is_query=False)
    
    @property
    def dimension(self) -> int:
        return self._dimension
    
    @property
    def model_name(self) -> str:
        return self._model_name
```

### 2.3 Hybrid Approach: Query Translation

```python
# src/core/rag/multilingual_retriever.py

from typing import List, Optional
import asyncio


class MultilingualRetriever:
    """
    ë‹¤êµ­ì–´ ê²€ìƒ‰ì„ ìœ„í•œ í•˜ì´ë¸Œë¦¬ë“œ ì ‘ê·¼.
    1. Multilingual embeddingìœ¼ë¡œ ì§ì ‘ ê²€ìƒ‰
    2. Query translationìœ¼ë¡œ ì¶”ê°€ ê²€ìƒ‰ (optional)
    """
    
    def __init__(
        self,
        store: ChromaStore,
        translator: Optional["QueryTranslator"] = None,
        enable_translation: bool = False
    ):
        self._store = store
        self._translator = translator
        self._enable_translation = enable_translation
    
    def retrieve(
        self,
        query: str,
        top_k: int = 5,
        languages: List[str] = ["ko", "en"]
    ) -> RetrievalResult:
        """
        ë‹¤êµ­ì–´ í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰.
        """
        all_results = []
        
        # 1. ì›ë³¸ ì¿¼ë¦¬ë¡œ ê²€ìƒ‰
        primary_results = self._store.query(query, n_results=top_k)
        all_results.extend(primary_results)
        
        # 2. ë²ˆì—­ëœ ì¿¼ë¦¬ë¡œ ì¶”ê°€ ê²€ìƒ‰ (optional)
        if self._enable_translation and self._translator:
            for lang in languages:
                translated = self._translator.translate(query, target_lang=lang)
                if translated != query:
                    secondary_results = self._store.query(translated, n_results=top_k // 2)
                    all_results.extend(secondary_results)
        
        # 3. ì¤‘ë³µ ì œê±° ë° ì ìˆ˜ ê¸°ë°˜ ì •ë ¬
        deduplicated = self._deduplicate_by_id(all_results)
        sorted_results = sorted(deduplicated, key=lambda x: x["distance"])
        
        return self._format_results(query, sorted_results[:top_k])
```

### 2.4 ChromaStore ìˆ˜ì • ì œì•ˆ

```python
# ê¸°ì¡´ ChromaStoreì— query/document prefix ì§€ì› ì¶”ê°€

class ChromaStore:
    def __init__(
        self,
        persist_path: str = "./chroma_db",
        collection_name: str = "obsidian_notes",
        embedder: Optional[EmbeddingStrategy] = None,
        use_instruction_prefix: bool = True  # ìƒˆ íŒŒë¼ë¯¸í„°
    ):
        self._use_instruction_prefix = use_instruction_prefix
        # ... ê¸°ì¡´ ì½”ë“œ
    
    def query(self, query_text: str, n_results: int = 5, **kwargs):
        """ì¿¼ë¦¬ ì‹œ instruction prefix ìë™ ì ìš©"""
        if self._use_instruction_prefix and hasattr(self._embedder, 'embed_query'):
            # Multilingual E5 ë“± instruction ëª¨ë¸ìš©
            query_embedding = self._embedder.embed_query(query_text)
            return self._collection.query(
                query_embeddings=[query_embedding],
                n_results=n_results,
                **kwargs
            )
        else:
            # ê¸°ì¡´ ë°©ì‹
            return self._collection.query(
                query_texts=[query_text],
                n_results=n_results,
                **kwargs
            )
```

---

## 3. GraphRAG ë° ì§€ì‹ ê·¸ë˜í”„

### 3.0 ìš©ì–´ ì •ë¦¬: Obsidian ê·¸ë˜í”„ ë·° vs GraphRAG/ê·¸ë˜í”„ ì„ë² ë”©

- **Obsidian ê·¸ë˜í”„ ë·°(ë¡œì»¬ ê·¸ë˜í”„/íƒœê·¸ ê·¸ë˜í”„)**: ë…¸íŠ¸/ë§í¬/íƒœê·¸ ê´€ê³„ë¥¼ **ì‹œê°í™”(UI)** í•´ì„œ ì‚¬ëŒì´ íƒìƒ‰í•˜ê¸° ì‰½ê²Œ ë³´ì—¬ì£¼ëŠ” ê¸°ëŠ¥.
- **ì§€ì‹ ê·¸ë˜í”„(Knowledge Graph)**: ë…¸íŠ¸/ì„¹ì…˜/íƒœê·¸/ì—”í‹°í‹°ë¥¼ **ë…¸ë“œ**, ê·¸ ê´€ê³„ë¥¼ **ì—£ì§€**ë¡œ ì €ì¥í•œ ë°ì´í„° êµ¬ì¡°(ê²€ìƒ‰Â·ì¶”ë¡ ì— ì‚¬ìš©).
- **ê·¸ë˜í”„ ì„ë² ë”©(Graph Embedding)**: ê·¸ë˜í”„ì˜ ë…¸ë“œ/ì—£ì§€ë¥¼ ëª¨ë¸ì´ ë‹¤ë£¨ê¸° ì‰¬ìš´ **ë²¡í„°(ìˆ«ì ë°°ì—´)** ë¡œ ë³€í™˜í•´, ìœ ì‚¬ë„ ê²€ìƒ‰Â·ì¶”ì²œÂ·ì»¤ë®¤ë‹ˆí‹° íƒì§€Â·ì¬ë­í‚¹ ë“±ì— í™œìš©í•˜ëŠ” ê¸°ë²•.
- **GraphRAG**: (1) ë¬¸ì„œì—ì„œ ì—”í‹°í‹°/ê´€ê³„ë¥¼ ì¶”ì¶œí•´ ì§€ì‹ ê·¸ë˜í”„ë¥¼ ë§Œë“¤ê³ , (2) ë²¡í„° ê²€ìƒ‰ + ê·¸ë˜í”„ íƒìƒ‰ì„ ê²°í•©í•´ ì»¨í…ìŠ¤íŠ¸ë¥¼ ê°•í™”í•˜ëŠ” RAG íŒ¨í„´.

> âœ… **ì¤‘ìš”:** GraphRAG/ê·¸ë˜í”„ ì„ë² ë”©ì€ â€œê·¸ë˜í”„ë¥¼ ê·¸ë ¤ì£¼ëŠ” ê¸°ëŠ¥â€ì´ ì•„ë‹ˆë¼, **ê·¸ë˜í”„(êµ¬ì¡° ë°ì´í„°)ë¥¼ ê²€ìƒ‰/ì¶”ë¡  ì„±ëŠ¥ í–¥ìƒì— ì“°ëŠ” ë°©ë²•**ì…ë‹ˆë‹¤.  
> âœ… **ìœ„í‚¤ë§í¬ë¥¼ ë§ì´ ì•ˆ ì¨ë„ ê°€ëŠ¥:** íƒœê·¸(`#tag`), í—¤ë”(`#`, `##`), íŒŒì¼ ê²½ë¡œ/í´ë”, LLM ê¸°ë°˜ ì—”í‹°í‹° ì¶”ì¶œë¡œë„ ê·¸ë˜í”„ë¥¼ ë§Œë“¤ ìˆ˜ ìˆìŠµë‹ˆë‹¤.

### 3.1 GraphRAG ê°œìš”

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                      GraphRAG Architecture                    â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                               â”‚
â”‚  Documents â”€â”€â–¶ Entity Extraction â”€â”€â–¶ Knowledge Graph         â”‚
â”‚                      â”‚                      â”‚                 â”‚
â”‚                      â–¼                      â–¼                 â”‚
â”‚              Relationship Extraction   Community Detection    â”‚
â”‚                      â”‚                      â”‚                 â”‚
â”‚                      â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                                 â–¼                             â”‚
â”‚                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                    â”‚
â”‚                    â”‚   Hybrid Search     â”‚                    â”‚
â”‚                    â”‚  Vector + Graph     â”‚                    â”‚
â”‚                    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜                    â”‚
â”‚                                 â”‚                             â”‚
â”‚                                 â–¼                             â”‚
â”‚                          Enhanced Context                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 3.2 Obsidian íŠ¹í™” Entity Extraction

Obsidianì˜ êµ¬ì¡°ë¥¼ í™œìš©í•œ ì—”í‹°í‹° ì¶”ì¶œ:

```python
# src/core/graph/obsidian_entity_extractor.py

import re
from dataclasses import dataclass
from typing import List, Tuple


@dataclass
class Entity:
    name: str
    type: str  # concept, person, tool, etc.
    description: str
    source_file: str


@dataclass
class Relationship:
    source: str
    target: str
    type: str  # links_to, mentions, related_to
    weight: float


class ObsidianEntityExtractor:
    """
    Obsidian ë§ˆí¬ë‹¤ìš´ì—ì„œ ì—”í‹°í‹°ì™€ ê´€ê³„ ì¶”ì¶œ.
    Obsidian íŠ¹ìœ ì˜ ë¬¸ë²• í™œìš©.
    """
    
    # Obsidian ë¬¸ë²• íŒ¨í„´
    WIKILINK_PATTERN = r'\[\[([^\]|]+)(?:\|[^\]]+)?\]\]'
    TAG_PATTERN = r'#([a-zA-Zê°€-í£][a-zA-Z0-9ê°€-í£_/-]*)'
    HEADER_PATTERN = r'^(#{1,6})\s+(.+)$'
    
    def extract_from_markdown(
        self,
        content: str,
        source_file: str
    ) -> Tuple[List[Entity], List[Relationship]]:
        """ë§ˆí¬ë‹¤ìš´ì—ì„œ ì—”í‹°í‹°ì™€ ê´€ê³„ ì¶”ì¶œ"""
        entities = []
        relationships = []
        
        # 1. Wikilinks â†’ Entity + Relationship
        wikilinks = re.findall(self.WIKILINK_PATTERN, content)
        for link in wikilinks:
            entities.append(Entity(
                name=link,
                type="linked_note",
                description=f"Linked from {source_file}",
                source_file=source_file
            ))
            relationships.append(Relationship(
                source=source_file,
                target=link,
                type="links_to",
                weight=1.0
            ))
        
        # 2. Tags â†’ Entity (category)
        tags = re.findall(self.TAG_PATTERN, content)
        for tag in tags:
            tag_name = f"#{tag}"
            entities.append(Entity(
                name=tag_name,
                type="tag",
                description=f"Tag used in {source_file}",
                source_file=source_file
            ))
            # ìœ„í‚¤ë§í¬ê°€ ì—†ì–´ë„ ê·¸ë˜í”„ë¥¼ ë§Œë“¤ ìˆ˜ ìˆë„ë¡ ê´€ê³„ë¥¼ ì¶”ê°€
            relationships.append(Relationship(
                source=source_file,
                target=tag_name,
                type="tagged_with",
                weight=0.6
            ))

        # 3. Headers â†’ Entity (concept)
        for match in re.finditer(self.HEADER_PATTERN, content, re.MULTILINE):
            level, header_text = match.groups()
            concept = header_text.strip()
            entities.append(Entity(
                name=concept,
                type="concept",
                description=f"Section header in {source_file}",
                source_file=source_file
            ))
            relationships.append(Relationship(
                source=source_file,
                target=concept,
                type="has_section",
                weight=0.4
            ))

        return entities, relationships
    
    def extract_with_llm(
        self,
        content: str,
        source_file: str,
        llm: "LLMStrategy"
    ) -> Tuple[List[Entity], List[Relationship]]:
        """LLMì„ í™œìš©í•œ ì‹¬ì¸µ ì—”í‹°í‹° ì¶”ì¶œ"""
        prompt = f"""
        ë‹¤ìŒ ë§ˆí¬ë‹¤ìš´ ë¬¸ì„œì—ì„œ ì£¼ìš” ì—”í‹°í‹°ì™€ ê´€ê³„ë¥¼ ì¶”ì¶œí•˜ì„¸ìš”.
        
        ë¬¸ì„œ:
        {content[:2000]}
        
        ì‘ë‹µ í˜•ì‹ (JSON):
        {{
            "entities": [
                {{"name": "...", "type": "person|concept|tool|organization", "description": "..."}}
            ],
            "relationships": [
                {{"source": "...", "target": "...", "type": "...", "description": "..."}}
            ]
        }}
        """
        # LLM í˜¸ì¶œ ë° íŒŒì‹±
        # ...
```

### 3.3 Graph Storage: Neo4j vs NetworkX

| ìš©ë„ | ê¶Œì¥ | ì´ìœ  |
|------|------|------|
| ê°œë°œ/í”„ë¡œí† íƒ€ì´í•‘ | NetworkX | ì„¤ì¹˜ ê°„ë‹¨, ë©”ëª¨ë¦¬ ê¸°ë°˜ |
| í”„ë¡œë•ì…˜ (<10K ë…¸ë“œ) | SQLite + JSON | ê°„ë‹¨, ë³„ë„ ì„œë²„ ë¶ˆí•„ìš” |
| í”„ë¡œë•ì…˜ (>10K ë…¸ë“œ) | Neo4j | ì¿¼ë¦¬ ìµœì í™”, í™•ì¥ì„± |

#### ê°„ë‹¨í•œ Graph Store (SQLite ê¸°ë°˜)

```python
# src/db/graph_store.py

import sqlite3
import json
from pathlib import Path
from typing import List, Dict, Any


class SimpleGraphStore:
    """
    SQLite ê¸°ë°˜ ê°„ë‹¨í•œ ê·¸ë˜í”„ ì €ì¥ì†Œ.
    ì†Œê·œëª¨ Obsidian vaultì— ì í•©.
    """
    
    def __init__(self, db_path: str = "./graph.db"):
        self.db_path = Path(db_path)
        self._init_db()
    
    def _init_db(self):
        with sqlite3.connect(self.db_path) as conn:
            conn.executescript("""
                CREATE TABLE IF NOT EXISTS entities (
                    id TEXT PRIMARY KEY,
                    name TEXT NOT NULL,
                    type TEXT,
                    description TEXT,
                    properties TEXT,  -- JSON
                    embedding BLOB    -- Optional: entity embedding
                );
                
                CREATE TABLE IF NOT EXISTS relationships (
                    id TEXT PRIMARY KEY,
                    source_id TEXT NOT NULL,
                    target_id TEXT NOT NULL,
                    type TEXT,
                    weight REAL DEFAULT 1.0,
                    properties TEXT,  -- JSON
                    FOREIGN KEY (source_id) REFERENCES entities(id),
                    FOREIGN KEY (target_id) REFERENCES entities(id)
                );
                
                CREATE INDEX IF NOT EXISTS idx_rel_source ON relationships(source_id);
                CREATE INDEX IF NOT EXISTS idx_rel_target ON relationships(target_id);
                CREATE INDEX IF NOT EXISTS idx_entity_type ON entities(type);
            """)
    
    def add_entity(self, entity: Dict[str, Any]) -> str:
        """ì—”í‹°í‹° ì¶”ê°€/ì—…ë°ì´íŠ¸"""
        entity_id = entity.get("id") or self._generate_id(entity["name"])
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                INSERT OR REPLACE INTO entities (id, name, type, description, properties)
                VALUES (?, ?, ?, ?, ?)
            """, (
                entity_id,
                entity["name"],
                entity.get("type"),
                entity.get("description"),
                json.dumps(entity.get("properties", {}))
            ))
        return entity_id
    
    def add_relationship(self, rel: Dict[str, Any]) -> str:
        """ê´€ê³„ ì¶”ê°€"""
        rel_id = f"{rel['source']}--{rel['type']}--{rel['target']}"
        with sqlite3.connect(self.db_path) as conn:
            conn.execute("""
                INSERT OR REPLACE INTO relationships 
                (id, source_id, target_id, type, weight, properties)
                VALUES (?, ?, ?, ?, ?, ?)
            """, (
                rel_id,
                rel["source"],
                rel["target"],
                rel.get("type", "related_to"),
                rel.get("weight", 1.0),
                json.dumps(rel.get("properties", {}))
            ))
        return rel_id
    
    def get_neighbors(self, entity_id: str, depth: int = 1) -> List[Dict]:
        """ì—”í‹°í‹°ì˜ ì´ì›ƒ ë…¸ë“œ ì¡°íšŒ"""
        with sqlite3.connect(self.db_path) as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.execute("""
                SELECT e.*, r.type as rel_type, r.weight
                FROM entities e
                JOIN relationships r ON (r.target_id = e.id OR r.source_id = e.id)
                WHERE r.source_id = ? OR r.target_id = ?
            """, (entity_id, entity_id))
            return [dict(row) for row in cursor.fetchall()]
    
    def get_entity_by_name(self, name: str) -> Dict | None:
        """ì´ë¦„ìœ¼ë¡œ ì—”í‹°í‹° ê²€ìƒ‰"""
        with sqlite3.connect(self.db_path) as conn:
            conn.row_factory = sqlite3.Row
            cursor = conn.execute(
                "SELECT * FROM entities WHERE name = ?", (name,)
            )
            row = cursor.fetchone()
            return dict(row) if row else None
```

### 3.4 Graph Visualization (Frontend)

```typescript
// front/components/knowledge-graph.tsx

"use client";

import { useEffect, useRef } from "react";
import cytoscape from "cytoscape";

interface GraphNode {
  id: string;
  label: string;
  type: string;
}

interface GraphEdge {
  source: string;
  target: string;
  label: string;
}

interface KnowledgeGraphProps {
  nodes: GraphNode[];
  edges: GraphEdge[];
  onNodeClick?: (nodeId: string) => void;
}

export function KnowledgeGraph({ nodes, edges, onNodeClick }: KnowledgeGraphProps) {
  const containerRef = useRef<HTMLDivElement>(null);
  const cyRef = useRef<cytoscape.Core | null>(null);

  useEffect(() => {
    if (!containerRef.current) return;

    const elements = [
      // Nodes
      ...nodes.map((node) => ({
        data: { id: node.id, label: node.label, type: node.type },
      })),
      // Edges
      ...edges.map((edge) => ({
        data: {
          id: `${edge.source}-${edge.target}`,
          source: edge.source,
          target: edge.target,
          label: edge.label,
        },
      })),
    ];

    cyRef.current = cytoscape({
      container: containerRef.current,
      elements,
      style: [
        {
          selector: "node",
          style: {
            "background-color": "#6366f1",
            label: "data(label)",
            color: "#fff",
            "text-valign": "center",
            "text-halign": "center",
            "font-size": "12px",
          },
        },
        {
          selector: "node[type='tag']",
          style: { "background-color": "#22c55e" },
        },
        {
          selector: "node[type='concept']",
          style: { "background-color": "#f59e0b" },
        },
        {
          selector: "edge",
          style: {
            width: 2,
            "line-color": "#94a3b8",
            "target-arrow-color": "#94a3b8",
            "target-arrow-shape": "triangle",
            "curve-style": "bezier",
          },
        },
      ],
      layout: { name: "cose", animate: true },
    });

    if (onNodeClick) {
      cyRef.current.on("tap", "node", (evt) => {
        onNodeClick(evt.target.id());
      });
    }

    return () => {
      cyRef.current?.destroy();
    };
  }, [nodes, edges, onNodeClick]);

  return (
    <div
      ref={containerRef}
      className="w-full h-[500px] border rounded-lg bg-slate-50"
    />
  );
}
```

### 3.5 Hybrid Search: Vector + Graph

```python
# src/core/rag/hybrid_graph_retriever.py

from typing import List, Set
from dataclasses import dataclass


@dataclass
class HybridResult:
    chunk_id: str
    text: str
    score: float
    graph_context: List[str]  # ì—°ê²°ëœ ì—”í‹°í‹°ë“¤


class HybridGraphRetriever:
    """
    Vector Search + Graph Traversal í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰.
    """
    
    def __init__(
        self,
        vector_store: "ChromaStore",
        graph_store: "SimpleGraphStore",
        graph_weight: float = 0.3
    ):
        self._vector_store = vector_store
        self._graph_store = graph_store
        self._graph_weight = graph_weight
    
    def retrieve(
        self,
        query: str,
        top_k: int = 5,
        graph_depth: int = 1
    ) -> List[HybridResult]:
        """
        1. Vector searchë¡œ ì´ˆê¸° ê²°ê³¼ íšë“
        2. ê²°ê³¼ì—ì„œ ì—”í‹°í‹° ì¶”ì¶œ
        3. Graph traversalë¡œ ê´€ë ¨ ì—”í‹°í‹° í™•ì¥
        4. í™•ì¥ëœ ì»¨í…ìŠ¤íŠ¸ í¬í•¨í•˜ì—¬ ë°˜í™˜
        """
        # 1. Vector search
        vector_results = self._vector_store.query(query, n_results=top_k * 2)
        
        # 2. ê²°ê³¼ì—ì„œ ì—”í‹°í‹° ì¶”ì¶œ
        entities: Set[str] = set()
        for result in vector_results:
            source = result["metadata"].get("source", "")
            if source:
                entities.add(source.replace(".md", ""))
        
        # 3. Graph traversal
        expanded_entities = set()
        for entity_name in entities:
            entity = self._graph_store.get_entity_by_name(entity_name)
            if entity:
                neighbors = self._graph_store.get_neighbors(
                    entity["id"], depth=graph_depth
                )
                for neighbor in neighbors:
                    expanded_entities.add(neighbor["name"])
        
        # 4. ê²°ê³¼ êµ¬ì„±
        results = []
        for result in vector_results[:top_k]:
            graph_context = list(expanded_entities)[:5]  # ìƒìœ„ 5ê°œ
            results.append(HybridResult(
                chunk_id=result["id"],
                text=result["text"],
                score=1 / (1 + result["distance"]),
                graph_context=graph_context
            ))
        
        return results
```

---

## 4. Agentic RAG íŒ¨í„´

### 4.1 Simple RAG vs Agentic RAG

```
Simple RAG (í˜„ì¬):
Query â†’ Vector Search â†’ Context â†’ LLM â†’ Response

Agentic RAG:
Query â†’ Conversation Analysis â†’ Query Clarification â†’ 
     â†’ Parallel Retrieval â†’ Self-Correction â†’ Response Synthesis
```

### 4.2 í•µì‹¬ Agentic íŒ¨í„´

#### íŒ¨í„´ 1: Query Rewriting

```python
# src/core/rag/agentic/query_rewriter.py

from typing import List, Tuple


class QueryRewriter:
    """
    ëŒ€í™” ì»¨í…ìŠ¤íŠ¸ë¥¼ ê³ ë ¤í•œ ì¿¼ë¦¬ ì¬ì‘ì„±.
    - ëª¨í˜¸í•œ ì°¸ì¡° í•´ê²° (ì˜ˆ: "ê·¸ê²ƒ" â†’ "API ì¸ì¦ ë°©ë²•")
    - ë³µì¡í•œ ì§ˆë¬¸ ë¶„í• 
    """
    
    REWRITE_PROMPT = """
    ë‹¹ì‹ ì€ ì¿¼ë¦¬ ë¶„ì„ ì „ë¬¸ê°€ì…ë‹ˆë‹¤.
    
    ëŒ€í™” ì´ë ¥:
    {history}
    
    í˜„ì¬ ì§ˆë¬¸:
    {query}
    
    ê·œì¹™:
    1. ì§ˆë¬¸ì´ ëª¨í˜¸í•˜ë©´ ëŒ€í™” ì´ë ¥ì„ ì°¸ì¡°í•´ ëª…í™•í•˜ê²Œ ì¬ì‘ì„±
    2. ë³µì¡í•œ ì§ˆë¬¸ì€ ìµœëŒ€ 3ê°œì˜ í•˜ìœ„ ì§ˆë¬¸ìœ¼ë¡œ ë¶„í• 
    3. ë¶„í•  ë¶ˆí•„ìš”ì‹œ ì›ë³¸ ì§ˆë¬¸ ê·¸ëŒ€ë¡œ ë°˜í™˜
    
    ì‘ë‹µ í˜•ì‹ (JSON):
    {{
        "is_clear": true/false,
        "rewritten_queries": ["ì§ˆë¬¸1", "ì§ˆë¬¸2", ...],
        "clarification_needed": "ëª…í™•í™” í•„ìš”ì‹œ ìš”ì²­í•  ë‚´ìš© (ì—†ìœ¼ë©´ null)"
    }}
    """
    
    def __init__(self, llm: "LLMStrategy"):
        self._llm = llm
    
    def rewrite(
        self,
        query: str,
        history: List["Message"] = None
    ) -> Tuple[bool, List[str], str | None]:
        """
        ì¿¼ë¦¬ ì¬ì‘ì„±.
        
        Returns:
            (is_clear, rewritten_queries, clarification_request)
        """
        history_text = self._format_history(history or [])
        prompt = self.REWRITE_PROMPT.format(
            history=history_text,
            query=query
        )
        
        response = self._llm.generate([
            {"role": "user", "content": prompt}
        ])
        
        result = self._parse_response(response.content)
        return (
            result.get("is_clear", True),
            result.get("rewritten_queries", [query]),
            result.get("clarification_needed")
        )
```

#### íŒ¨í„´ 2: Self-Correction (Retry Logic)

```python
# src/core/rag/agentic/self_correcting_chain.py

from dataclasses import dataclass
from typing import List, Optional


@dataclass
class CorrectionResult:
    answer: str
    attempts: int
    final_query: str
    retrieval_quality: float


class SelfCorrectingRAGChain:
    """
    ê²€ìƒ‰ ê²°ê³¼ê°€ ë¶ˆì¶©ë¶„í•  ë•Œ ìë™ìœ¼ë¡œ ì¬ì‹œë„í•˜ëŠ” RAG ì²´ì¸.
    """
    
    QUALITY_THRESHOLD = 0.6
    MAX_RETRIES = 2
    
    def __init__(
        self,
        retriever: "Retriever",
        llm: "LLMStrategy",
        query_rewriter: "QueryRewriter"
    ):
        self._retriever = retriever
        self._llm = llm
        self._query_rewriter = query_rewriter
    
    def query(self, question: str, **kwargs) -> CorrectionResult:
        """
        Self-correcting RAG ì¿¼ë¦¬.
        
        1. ì´ˆê¸° ê²€ìƒ‰ ìˆ˜í–‰
        2. ê²°ê³¼ í’ˆì§ˆ í‰ê°€
        3. í’ˆì§ˆ ë‚®ìœ¼ë©´ ì¿¼ë¦¬ ì¬ì‘ì„± í›„ ì¬ì‹œë„
        """
        current_query = question
        attempts = 0
        
        while attempts <= self.MAX_RETRIES:
            attempts += 1
            
            # ê²€ìƒ‰ ìˆ˜í–‰
            result = self._retriever.retrieve(current_query, top_k=5)
            
            # í’ˆì§ˆ í‰ê°€
            quality = self._evaluate_quality(result, current_query)
            
            if quality >= self.QUALITY_THRESHOLD:
                # ì¶©ë¶„í•œ í’ˆì§ˆ â†’ ì‘ë‹µ ìƒì„±
                answer = self._generate_answer(current_query, result)
                return CorrectionResult(
                    answer=answer,
                    attempts=attempts,
                    final_query=current_query,
                    retrieval_quality=quality
                )
            
            if attempts <= self.MAX_RETRIES:
                # ì¿¼ë¦¬ ì¬ì‘ì„±
                current_query = self._broaden_query(current_query)
        
        # ìµœëŒ€ ì¬ì‹œë„ í›„ì—ë„ ì‹¤íŒ¨ â†’ ìµœì„ ì˜ ê²°ê³¼ë¡œ ì‘ë‹µ
        answer = self._generate_answer(current_query, result)
        return CorrectionResult(
            answer=answer,
            attempts=attempts,
            final_query=current_query,
            retrieval_quality=quality
        )
    
    def _evaluate_quality(
        self,
        result: "RetrievalResult",
        query: str
    ) -> float:
        """ê²€ìƒ‰ ê²°ê³¼ í’ˆì§ˆ í‰ê°€ (0~1)"""
        if not result.chunks:
            return 0.0
        
        # ìƒìœ„ ê²°ê³¼ì˜ í‰ê·  score
        top_scores = [c.score for c in result.chunks[:3]]
        avg_score = sum(top_scores) / len(top_scores)
        
        return avg_score
    
    def _broaden_query(self, query: str) -> str:
        """ì¿¼ë¦¬ë¥¼ ë” ë„“ì€ ë²”ìœ„ë¡œ ì¬ì‘ì„±"""
        prompt = f"""
        ë‹¤ìŒ ê²€ìƒ‰ ì¿¼ë¦¬ê°€ ê²°ê³¼ë¥¼ ì°¾ì§€ ëª»í–ˆìŠµë‹ˆë‹¤.
        ë” ë„“ì€ ë²”ìœ„ë¡œ ì¬ì‘ì„±í•´ì£¼ì„¸ìš”.
        
        ì›ë³¸ ì¿¼ë¦¬: {query}
        
        ì¬ì‘ì„±ëœ ì¿¼ë¦¬:
        """
        response = self._llm.generate([{"role": "user", "content": prompt}])
        return response.content.strip()
```

#### íŒ¨í„´ 3: Parallel Query Processing

```python
# src/core/rag/agentic/parallel_retriever.py

import asyncio
from typing import List
from concurrent.futures import ThreadPoolExecutor


class ParallelQueryProcessor:
    """
    ë³µì¡í•œ ì§ˆë¬¸ì„ ë¶„í• í•˜ì—¬ ë³‘ë ¬ ì²˜ë¦¬.
    """
    
    def __init__(
        self,
        retriever: "Retriever",
        max_workers: int = 3
    ):
        self._retriever = retriever
        self._executor = ThreadPoolExecutor(max_workers=max_workers)
    
    def process_queries(
        self,
        queries: List[str],
        top_k: int = 5
    ) -> List["RetrievalResult"]:
        """ì—¬ëŸ¬ ì¿¼ë¦¬ ë³‘ë ¬ ì²˜ë¦¬"""
        futures = [
            self._executor.submit(self._retriever.retrieve, q, top_k)
            for q in queries
        ]
        return [f.result() for f in futures]
    
    async def process_queries_async(
        self,
        queries: List[str],
        top_k: int = 5
    ) -> List["RetrievalResult"]:
        """Async ë²„ì „"""
        loop = asyncio.get_event_loop()
        tasks = [
            loop.run_in_executor(
                self._executor,
                self._retriever.retrieve,
                q,
                top_k
            )
            for q in queries
        ]
        return await asyncio.gather(*tasks)
    
    def aggregate_results(
        self,
        results: List["RetrievalResult"],
        top_k: int = 5
    ) -> "RetrievalResult":
        """ì—¬ëŸ¬ ê²°ê³¼ë¥¼ í•˜ë‚˜ë¡œ ë³‘í•©"""
        all_chunks = []
        seen_ids = set()
        
        for result in results:
            for chunk in result.chunks:
                if chunk.id not in seen_ids:
                    all_chunks.append(chunk)
                    seen_ids.add(chunk.id)
        
        # Scoreë¡œ ì •ë ¬
        all_chunks.sort(key=lambda c: c.score, reverse=True)
        
        return RetrievalResult(
            query="[merged]",
            chunks=all_chunks[:top_k],
            total_count=len(all_chunks)
        )
```

### 4.3 Hierarchical Indexing (Parent-Child Chunks)

```python
# src/core/preprocessing/hierarchical_chunker.py

from dataclasses import dataclass
from typing import List, Optional


@dataclass
class HierarchicalChunk:
    id: str
    text: str
    metadata: dict
    parent_id: Optional[str] = None
    children_ids: List[str] = None


class HierarchicalChunker:
    """
    Parent-Child ì²­í¬ ê³„ì¸µ êµ¬ì¡°.
    - Parent: í° ì„¹ì…˜ (í—¤ë” ê¸°ë°˜)
    - Child: ì‘ì€ ì²­í¬ (ê²€ìƒ‰ìš©, 500ì)
    
    ê²€ìƒ‰ ì‹œ: Childë¡œ ì •ë°€ ê²€ìƒ‰ â†’ Parentë¡œ ì»¨í…ìŠ¤íŠ¸ í™•ì¥
    """
    
    def __init__(
        self,
        parent_chunk_size: int = 2000,
        child_chunk_size: int = 500,
        child_overlap: int = 50
    ):
        self.parent_size = parent_chunk_size
        self.child_size = child_chunk_size
        self.child_overlap = child_overlap
    
    def chunk(
        self,
        text: str,
        source: str
    ) -> tuple[List[HierarchicalChunk], List[HierarchicalChunk]]:
        """
        Returns:
            (parent_chunks, child_chunks)
        """
        parents = []
        children = []
        
        # 1. Parent ì²­í¬ ìƒì„± (í—¤ë” ê¸°ë°˜)
        parent_chunks = self._create_parent_chunks(text, source)
        parents.extend(parent_chunks)
        
        # 2. ê° Parentì—ì„œ Child ì²­í¬ ìƒì„±
        for parent in parent_chunks:
            child_chunks = self._split_to_children(parent)
            children.extend(child_chunks)
        
        return parents, children
    
    def _create_parent_chunks(
        self,
        text: str,
        source: str
    ) -> List[HierarchicalChunk]:
        """í—¤ë” ê¸°ë°˜ Parent ì²­í¬ ìƒì„±"""
        # ê¸°ì¡´ semantic_chunk ë¡œì§ í™œìš©
        from core.preprocessing import semantic_chunk
        chunks = semantic_chunk(
            text=text,
            source=source,
            min_size=500,
            max_size=self.parent_size
        )
        return [
            HierarchicalChunk(
                id=f"{source}::parent_{i}",
                text=c.text,
                metadata=c.metadata,
                children_ids=[]
            )
            for i, c in enumerate(chunks)
        ]
    
    def _split_to_children(
        self,
        parent: HierarchicalChunk
    ) -> List[HierarchicalChunk]:
        """Parentë¥¼ ì‘ì€ Child ì²­í¬ë¡œ ë¶„í• """
        text = parent.text
        children = []
        
        start = 0
        idx = 0
        while start < len(text):
            end = min(start + self.child_size, len(text))
            child_text = text[start:end]
            
            child = HierarchicalChunk(
                id=f"{parent.id}::child_{idx}",
                text=child_text,
                metadata={**parent.metadata, "parent_id": parent.id},
                parent_id=parent.id
            )
            children.append(child)
            parent.children_ids.append(child.id)
            
            start = end - self.child_overlap
            idx += 1
        
        return children
```

---

## 5. Advanced RAG ê¸°ë²•

### 5.1 Hybrid Search (Dense + Sparse)

```python
# src/core/rag/hybrid_search.py

from rank_bm25 import BM25Okapi
import numpy as np
from typing import List


class HybridSearcher:
    """
    Dense (Vector) + Sparse (BM25) í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰.
    """
    
    def __init__(
        self,
        vector_store: "ChromaStore",
        dense_weight: float = 0.7,
        sparse_weight: float = 0.3
    ):
        self._vector_store = vector_store
        self._dense_weight = dense_weight
        self._sparse_weight = sparse_weight
        self._bm25 = None
        self._documents = []
    
    def index_documents(self, documents: List[str], ids: List[str]):
        """BM25 ì¸ë±ìŠ¤ êµ¬ì¶•"""
        self._documents = documents
        self._doc_ids = ids
        tokenized = [doc.lower().split() for doc in documents]
        self._bm25 = BM25Okapi(tokenized)
    
    def search(self, query: str, top_k: int = 10) -> List[dict]:
        """í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰"""
        # 1. Dense search
        dense_results = self._vector_store.query(query, n_results=top_k * 2)
        dense_scores = {
            r["id"]: 1 / (1 + r["distance"])
            for r in dense_results
        }
        
        # 2. Sparse search (BM25)
        tokenized_query = query.lower().split()
        bm25_scores = self._bm25.get_scores(tokenized_query)
        
        # Normalize BM25 scores
        max_bm25 = max(bm25_scores) if max(bm25_scores) > 0 else 1
        sparse_scores = {
            self._doc_ids[i]: score / max_bm25
            for i, score in enumerate(bm25_scores)
        }
        
        # 3. Combine scores
        all_ids = set(dense_scores.keys()) | set(sparse_scores.keys())
        combined = []
        
        for doc_id in all_ids:
            d_score = dense_scores.get(doc_id, 0)
            s_score = sparse_scores.get(doc_id, 0)
            final_score = (
                self._dense_weight * d_score +
                self._sparse_weight * s_score
            )
            combined.append({
                "id": doc_id,
                "score": final_score,
                "dense_score": d_score,
                "sparse_score": s_score
            })
        
        # Sort by combined score
        combined.sort(key=lambda x: x["score"], reverse=True)
        return combined[:top_k]
```

### 5.2 Reranking with Cross-Encoder

```python
# src/core/rag/reranker.py

from sentence_transformers import CrossEncoder
from typing import List


class Reranker:
    """
    Cross-Encoder ê¸°ë°˜ ì¬ìˆœìœ„í™”.
    ì´ˆê¸° ê²€ìƒ‰ ê²°ê³¼ë¥¼ ë” ì •ë°€í•˜ê²Œ ì •ë ¬.
    """
    
    DEFAULT_MODEL = "cross-encoder/ms-marco-MiniLM-L-6-v2"
    
    def __init__(self, model_name: str = DEFAULT_MODEL):
        self._model_name = model_name
        self._model = None
    
    def _load_model(self):
        if self._model is None:
            self._model = CrossEncoder(self._model_name)
    
    def rerank(
        self,
        query: str,
        documents: List[str],
        top_k: int = 5
    ) -> List[tuple[str, float]]:
        """
        ë¬¸ì„œ ì¬ìˆœìœ„í™”.
        
        Returns:
            [(document, score), ...] ì •ë ¬ëœ ë¦¬ìŠ¤íŠ¸
        """
        self._load_model()
        
        # Query-Document ìŒ ìƒì„±
        pairs = [(query, doc) for doc in documents]
        
        # Cross-Encoder ì ìˆ˜ ê³„ì‚°
        scores = self._model.predict(pairs)
        
        # ì •ë ¬
        doc_scores = list(zip(documents, scores))
        doc_scores.sort(key=lambda x: x[1], reverse=True)
        
        return doc_scores[:top_k]


class RerankedRetriever:
    """Rerankingì„ ì ìš©í•œ Retriever ë˜í¼"""
    
    def __init__(
        self,
        base_retriever: "Retriever",
        reranker: "Reranker",
        initial_k: int = 20
    ):
        self._base = base_retriever
        self._reranker = reranker
        self._initial_k = initial_k
    
    def retrieve(self, query: str, top_k: int = 5) -> "RetrievalResult":
        # 1. ì´ˆê¸° ê²€ìƒ‰ (ë” ë§ì´)
        initial_result = self._base.retrieve(query, top_k=self._initial_k)
        
        # 2. Rerank
        documents = [c.text for c in initial_result.chunks]
        reranked = self._reranker.rerank(query, documents, top_k=top_k)
        
        # 3. ê²°ê³¼ ì¬êµ¬ì„±
        reranked_chunks = []
        for doc_text, score in reranked:
            for chunk in initial_result.chunks:
                if chunk.text == doc_text:
                    chunk.score = score  # Update score
                    reranked_chunks.append(chunk)
                    break
        
        return RetrievalResult(
            query=query,
            chunks=reranked_chunks,
            total_count=len(reranked_chunks)
        )
```

### 5.3 Query Expansion

```python
# src/core/rag/query_expansion.py

from typing import List


class QueryExpander:
    """
    ì¿¼ë¦¬ í™•ì¥ìœ¼ë¡œ ê²€ìƒ‰ recall í–¥ìƒ.
    - ë™ì˜ì–´ ì¶”ê°€
    - ê´€ë ¨ ìš©ì–´ ìƒì„±
    """
    
    EXPANSION_PROMPT = """
    ë‹¤ìŒ ê²€ìƒ‰ ì¿¼ë¦¬ì— ëŒ€í•´ ê´€ë ¨ ê²€ìƒ‰ì–´ë¥¼ ìƒì„±í•˜ì„¸ìš”.
    ë™ì˜ì–´, ê´€ë ¨ ê°œë…, ë‹¤ë¥¸ í‘œí˜„ì„ í¬í•¨í•´ì£¼ì„¸ìš”.
    
    ì›ë³¸ ì¿¼ë¦¬: {query}
    
    í™•ì¥ ê²€ìƒ‰ì–´ (ì‰¼í‘œë¡œ êµ¬ë¶„, ìµœëŒ€ 5ê°œ):
    """
    
    def __init__(self, llm: "LLMStrategy"):
        self._llm = llm
    
    def expand(self, query: str, max_terms: int = 5) -> List[str]:
        """ì¿¼ë¦¬ë¥¼ í™•ì¥í•˜ì—¬ ê´€ë ¨ ê²€ìƒ‰ì–´ ìƒì„±"""
        prompt = self.EXPANSION_PROMPT.format(query=query)
        response = self._llm.generate([{"role": "user", "content": prompt}])
        
        # íŒŒì‹±
        terms = [t.strip() for t in response.content.split(",")]
        return [query] + terms[:max_terms]
    
    def expand_for_multilingual(
        self,
        query: str,
        languages: List[str] = ["ko", "en"]
    ) -> List[str]:
        """ë‹¤êµ­ì–´ í™•ì¥"""
        expanded = [query]
        
        for lang in languages:
            if lang == "ko":
                # í•œê¸€ ë™ì˜ì–´/ê´€ë ¨ì–´ ì¶”ê°€
                korean_terms = self._get_korean_synonyms(query)
                expanded.extend(korean_terms)
            elif lang == "en":
                # ì˜ì–´ ë™ì˜ì–´/ê´€ë ¨ì–´ ì¶”ê°€
                english_terms = self._get_english_synonyms(query)
                expanded.extend(english_terms)
        
        return list(set(expanded))  # ì¤‘ë³µ ì œê±°
```

---

## 6. êµ¬í˜„ ìš°ì„ ìˆœìœ„ ë° ë¡œë“œë§µ

### 6.1 ìš°ì„ ìˆœìœ„ ë§¤íŠ¸ë¦­ìŠ¤

| ê¸°ëŠ¥ | ì˜í–¥ë„ | ë³µì¡ë„ | ìš°ì„ ìˆœìœ„ | ì˜ˆìƒ ê¸°ê°„ |
|------|--------|--------|----------|----------|
| **Multilingual Embedding (E5)** | ğŸ”´ High | ğŸŸ¢ Low | ğŸ¥‡ 1ìˆœìœ„ | 1-2ì¼ |
| **Hybrid Search (Dense+Sparse)** | ğŸŸ¡ Medium | ğŸŸ¢ Low | ğŸ¥ˆ 2ìˆœìœ„ | 2-3ì¼ |
| **Reranking** | ğŸŸ¡ Medium | ğŸŸ¢ Low | ğŸ¥‰ 3ìˆœìœ„ | 1-2ì¼ |
| **Query Rewriting (Agentic)** | ğŸŸ¡ Medium | ğŸŸ¡ Medium | 4ìˆœìœ„ | 3-5ì¼ |
| **Self-Correction** | ğŸŸ¡ Medium | ğŸŸ¡ Medium | 5ìˆœìœ„ | 2-3ì¼ |
| **Graph Entity Extraction** | ğŸŸ¡ Medium | ğŸŸ¡ Medium | 6ìˆœìœ„ | 5-7ì¼ |
| **Graph Visualization** | ğŸŸ¢ Low | ğŸŸ¡ Medium | 7ìˆœìœ„ | 3-5ì¼ |
| **Full GraphRAG** | ğŸŸ¡ Medium | ğŸ”´ High | 8ìˆœìœ„ | 2-3ì£¼ |

### 6.2 Phaseë³„ ë¡œë“œë§µ

#### Phase 1: Core Improvements (1-2ì£¼)

```
Week 1:
â”œâ”€â”€ Day 1-2: Multilingual E5 Embedder êµ¬í˜„
â”œâ”€â”€ Day 3-4: Hybrid Search (BM25 ì¶”ê°€)
â””â”€â”€ Day 5: Reranker í†µí•©

Week 2:
â”œâ”€â”€ Day 1-2: Query prefix ì§€ì› (query:/passage:)
â”œâ”€â”€ Day 3-4: í…ŒìŠ¤íŠ¸ ë° ë²¤ì¹˜ë§ˆí¬
â””â”€â”€ Day 5: ë¬¸ì„œí™”
```

**ì‚°ì¶œë¬¼:**
- `MultilingualE5Embedder` í´ë˜ìŠ¤
- `HybridSearcher` í´ë˜ìŠ¤
- `Reranker` í´ë˜ìŠ¤
- í•œì˜ êµì°¨ ê²€ìƒ‰ ì„±ëŠ¥ ê°œì„ 

#### Phase 2: Agentic Features (2-3ì£¼)

```
Week 3-4:
â”œâ”€â”€ Query Rewriting ì‹œìŠ¤í…œ
â”œâ”€â”€ Self-Correction ë¡œì§
â”œâ”€â”€ Parallel Query Processing
â””â”€â”€ Hierarchical Chunking (Parent-Child)
```

**ì‚°ì¶œë¬¼:**
- `QueryRewriter` í´ë˜ìŠ¤
- `SelfCorrectingRAGChain` í´ë˜ìŠ¤
- `ParallelQueryProcessor` í´ë˜ìŠ¤
- ë³µì¡í•œ ì§ˆë¬¸ ì²˜ë¦¬ ëŠ¥ë ¥ í–¥ìƒ

#### Phase 3: Graph Integration (3-4ì£¼)

```
Week 5-7:
â”œâ”€â”€ Obsidian Entity Extractor
â”œâ”€â”€ SimpleGraphStore (SQLite)
â”œâ”€â”€ Hybrid Graph Retriever
â”œâ”€â”€ Graph API Endpoints
â””â”€â”€ Frontend Graph Visualization
```

**ì‚°ì¶œë¬¼:**
- Knowledge Graph êµ¬ì¶• íŒŒì´í”„ë¼ì¸
- Graph + Vector í•˜ì´ë¸Œë¦¬ë“œ ê²€ìƒ‰
- Interactive Graph UI

### 6.3 ë¹ ë¥¸ ì‹œì‘: 1ìˆœìœ„ êµ¬í˜„

ì¦‰ì‹œ ì ìš© ê°€ëŠ¥í•œ Multilingual Embedding ë³€ê²½:

```python
# 1. requirements.txtì— ì¶”ê°€
# sentence-transformers>=2.2.0

# 2. src/core/embedding/__init__.py ìˆ˜ì •
from .multilingual_embedder import MultilingualE5Embedder

# 3. ì‚¬ìš©
embedder = MultilingualE5Embedder()
store = ChromaStore(embedder=embedder)

# í•œê¸€ë¡œ ì‘ì„±ëœ ë…¸íŠ¸, ì˜ì–´ë¡œ ê²€ìƒ‰ ê°€ëŠ¥
results = store.query("How to authenticate API?")  # í•œê¸€ ë…¸íŠ¸ë„ ê²€ìƒ‰ë¨
```

---

## ì°¸ê³  ìë£Œ

### ì—°êµ¬ ë…¼ë¬¸
- [BGE M3-Embedding](https://arxiv.org/abs/2402.03216) - Multi-Lingual, Multi-Functionality Text Embeddings
- [GraphRAG](https://arxiv.org/abs/2404.16130) - From Local to Global: A Graph RAG Approach
- [RankRAG](https://arxiv.org/abs/2407.02485) - Unifying Context Ranking with RAG

### ì˜¤í”ˆì†ŒìŠ¤ í”„ë¡œì íŠ¸
- [ApeRAG](https://github.com/apecloud/ApeRAG) - Production-grade RAG with Graph support
- [agentic-rag-for-dummies](https://github.com/GiovanniPasq/agentic-rag-for-dummies) - Agentic RAG patterns
- [Kotaemon](https://github.com/Cinnamon/kotaemon) - RAG-based QA with UI
- [Microsoft GraphRAG](https://github.com/microsoft/graphrag) - Official GraphRAG implementation

### ëª¨ë¸
- [intfloat/multilingual-e5-large-instruct](https://huggingface.co/intfloat/multilingual-e5-large-instruct)
- [BAAI/bge-m3](https://huggingface.co/BAAI/bge-m3)
- [dragonkue/BGE-m3-ko](https://huggingface.co/dragonkue/BGE-m3-ko)
