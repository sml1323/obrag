"""
Chat Router ë””ë²„ê¹…ìš© ìŠ¤í¬ë¦½íŠ¸

API ì—”ë“œí¬ì¸íŠ¸ë¥¼ ê±°ì¹˜ì§€ ì•Šê³  RAG íŒŒì´í”„ë¼ì¸ ë‚´ë¶€ë¥¼ ì§ì ‘ íƒìƒ‰í•©ë‹ˆë‹¤.
VSCode/PyCharmì—ì„œ breakpoint ê±¸ê³  F5ë¡œ ë””ë²„ê¹…í•˜ì„¸ìš”.

ì‚¬ìš©ë²•:
    python -m tasktests.phase2.debug_chat
    ë˜ëŠ” IDEì—ì„œ ì´ íŒŒì¼ì„ ë””ë²„ê·¸ ì‹¤í–‰
"""

import sys
from pathlib import Path

# í”„ë¡œì íŠ¸ ë£¨íŠ¸ë¥¼ pathì— ì¶”ê°€
project_root = Path(__file__).parent.parent.parent
sys.path.insert(0, str(project_root))

from dotenv import load_dotenv

load_dotenv(project_root / ".env")

from core.rag import RAGChain, Retriever, PromptBuilder
from core.llm import LLMFactory
from core.embedding import EmbedderFactory
from db.chroma_store import ChromaStore
from config.models import OpenAILLMConfig, OpenAIEmbeddingConfig
import os


def main():
    print("=" * 60)
    print("ğŸ” Chat Router ë””ë²„ê¹… ì‹œì‘")
    print("=" * 60)

    # ========================================
    # 1. ì˜ì¡´ì„± ì§ì ‘ êµ¬ì„±
    # ========================================
    print("\nğŸ“¦ [1] ì˜ì¡´ì„± ì´ˆê¸°í™”")
    print("-" * 40)

    # Embedder
    embed_config = OpenAIEmbeddingConfig(
        model_name=os.getenv("EMBEDDING_MODEL", "text-embedding-3-small")
    )
    embedder = EmbedderFactory.create(embed_config)
    print(f"  Embedder: {embed_config.model_name}")

    # ChromaStore
    chroma_path = os.getenv("CHROMA_PATH", "./chroma_db")
    chroma_store = ChromaStore(
        persist_path=chroma_path,
        collection_name="obsidian_notes",
        embedder=embedder,
    )
    print(f"  ChromaDB: {chroma_path}")

    # LLM
    llm_config = OpenAILLMConfig(model_name=os.getenv("LLM_MODEL", "gpt-4o-mini"))
    llm = LLMFactory.create(llm_config)
    print(f"  LLM: {llm_config.model_name}")

    # Retriever + RAGChain
    retriever = Retriever(chroma_store)
    chain = RAGChain(retriever=retriever, llm=llm)
    print("  RAGChain: ì´ˆê¸°í™” ì™„ë£Œ")

    # ========================================
    # 2. í…ŒìŠ¤íŠ¸ ì¿¼ë¦¬ ì„¤ì •
    # ========================================
    question = "Transformer ì•„í‚¤í…ì²˜ì— ëŒ€í•´ ì„¤ëª…í•´ì¤˜"  # ğŸ‘ˆ ì›í•˜ëŠ” ì§ˆë¬¸ìœ¼ë¡œ ë³€ê²½
    top_k = 5
    temperature = 0.7

    print(f"\nâ“ ì§ˆë¬¸: {question}")
    print(f"   top_k={top_k}, temperature={temperature}")

    # ========================================
    # 3. Step-by-Step íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
    # ========================================

    # ---- Step 3.1: Retriever.retrieve() ----
    print("\nğŸ” [3.1] Retriever.retrieve()")
    print("-" * 40)

    retrieval_result = retriever.retrieve(question, top_k=top_k)  # ğŸ‘ˆ breakpoint!

    print(f"  ê²€ìƒ‰ëœ ì²­í¬ ìˆ˜: {retrieval_result.total_count}")
    for i, chunk in enumerate(retrieval_result.chunks):
        print(f"\n  === Chunk {i + 1} ===")
        print(f"  ğŸ“ Source: {chunk.metadata.get('source', 'unknown')}")
        print(f"  ğŸ“Š Score: {chunk.score:.4f} (distance: {chunk.distance:.4f})")
        print(f"  ğŸ·ï¸ Headers: {chunk.metadata.get('headers', [])}")
        preview = chunk.text[:150].replace("\n", " ")
        print(f"  ğŸ“ Preview: {preview}...")

    # ---- Step 3.2: retrieve_with_context() ----
    print("\nğŸ“„ [3.2] retrieve_with_context()")
    print("-" * 40)

    context = retriever.retrieve_with_context(question, top_k=top_k)  # ğŸ‘ˆ breakpoint!

    print(f"  Context ê¸¸ì´: {len(context)} ë¬¸ì")
    print(f"  Context ë¯¸ë¦¬ë³´ê¸°:\n{context[:500]}...")

    # ---- Step 3.3: PromptBuilder.build() ----
    print("\nâœï¸ [3.3] PromptBuilder.build()")
    print("-" * 40)

    prompt_builder = chain.prompt_builder
    messages = prompt_builder.build(
        question=question, context=context
    )  # ğŸ‘ˆ breakpoint!

    for msg in messages:
        role = msg["role"]
        content = msg["content"]
        print(f"\n  [{role.upper()}]")
        if len(content) > 300:
            print(f"  {content[:300]}...")
            print(f"  ... (ì´ {len(content)} ë¬¸ì)")
        else:
            print(f"  {content}")

    # ---- Step 3.4: LLM.generate() (ë¹„ìš© ì£¼ì˜!) ----
    print("\nğŸ¤– [3.4] LLM.generate()")
    print("-" * 40)

    # âš ï¸ ì‹¤ì œ API í˜¸ì¶œë¨ - ë¹„ìš© ë°œìƒ!
    # ë””ë²„ê¹…ë§Œ í•  ë•ŒëŠ” ì´ ë¶€ë¶„ ì£¼ì„ì²˜ë¦¬ ê¶Œì¥

    CALL_LLM = True  # Falseë¡œ ë³€ê²½í•˜ë©´ LLM í˜¸ì¶œ ìŠ¤í‚µ

    if CALL_LLM:
        llm_response = llm.generate(messages, temperature=temperature)  # ğŸ‘ˆ breakpoint!

        print(f"  Model: {llm_response.model}")
        print(f"  Usage: {llm_response.usage}")
        print(f"\n  ğŸ“ Response:")
        print(f"  {llm_response.content[:500]}...")
    else:
        print("  â­ï¸ LLM í˜¸ì¶œ ìŠ¤í‚µë¨ (CALL_LLM=False)")

    # ========================================
    # 4. RAGChain.query() - ì „ì²´ íŒŒì´í”„ë¼ì¸
    # ========================================
    print("\nğŸš€ [4] RAGChain.query() - ì „ì²´ íŒŒì´í”„ë¼ì¸")
    print("-" * 40)

    if CALL_LLM:
        response = chain.query(
            question,
            top_k=top_k,
            temperature=temperature,
        )  # ğŸ‘ˆ breakpoint!

        print(f"  Model: {response.model}")
        print(f"  Usage: {response.usage}")
        print(f"  Sources: {len(response.retrieval_result.chunks)}ê°œ")
        print(f"\n  ğŸ“ Answer:")
        print(f"  {response.answer}")
    else:
        print("  â­ï¸ ì „ì²´ íŒŒì´í”„ë¼ì¸ ìŠ¤í‚µë¨ (CALL_LLM=False)")

    # ========================================
    # 5. ë©€í‹°í„´ ëŒ€í™” (ì„ íƒ)
    # ========================================
    print("\nğŸ’¬ [5] query_with_history() - ë©€í‹°í„´")
    print("-" * 40)

    if CALL_LLM:
        history = [
            {"role": "user", "content": "RAGê°€ ë­ì•¼?"},
            {
                "role": "assistant",
                "content": "RAGëŠ” Retrieval-Augmented Generationì˜ ì•½ìë¡œ...",
            },
        ]

        followup = "ê·¸ëŸ¼ ì„ë² ë”©ì€ ì–´ë–»ê²Œ ë™ì‘í•´?"

        response2 = chain.query_with_history(
            followup,
            history=history,
            top_k=3,
        )  # ğŸ‘ˆ breakpoint!

        print(f"  Follow-up ì§ˆë¬¸: {followup}")
        print(f"\n  ğŸ“ Answer:")
        print(f"  {response2.answer[:500]}...")
    else:
        print("  â­ï¸ ë©€í‹°í„´ ìŠ¤í‚µë¨ (CALL_LLM=False)")

    print("\n" + "=" * 60)
    print("âœ… ë””ë²„ê¹… ì™„ë£Œ!")
    print("=" * 60)


def debug_dynamic_chain():
    """
    _get_dynamic_chain() ë¡œì§ ë””ë²„ê¹….

    chat.pyì˜ ë™ì  ì²´ì¸ ìƒì„± ë¡œì§ì„ ë¶„ë¦¬í•´ì„œ í…ŒìŠ¤íŠ¸í•©ë‹ˆë‹¤.
    """
    print("\nğŸ”§ ë™ì  ì²´ì¸ ìƒì„± ë””ë²„ê¹…")
    print("=" * 60)

    from config.models import GeminiLLMConfig, OllamaLLMConfig

    # ë‹¤ì–‘í•œ provider í…ŒìŠ¤íŠ¸
    providers = [
        ("openai", "gpt-4o-mini", os.getenv("OPENAI_API_KEY")),
        ("gemini", "gemini-1.5-flash", os.getenv("GOOGLE_API_KEY")),
        # ("ollama", "llama3", None),  # ë¡œì»¬ Ollama í•„ìš”
    ]

    for provider, model, api_key in providers:
        print(f"\n  Testing: {provider}/{model}")
        try:
            if provider == "openai":
                config = OpenAILLMConfig(model_name=model, api_key=api_key)
            elif provider == "gemini":
                config = GeminiLLMConfig(model_name=model, api_key=api_key)
            elif provider == "ollama":
                config = OllamaLLMConfig(model_name=model)

            llm = LLMFactory.create(config)  # ğŸ‘ˆ breakpoint!
            print(f"    âœ… LLM ìƒì„± ì„±ê³µ: {llm.model_name}")
        except Exception as e:
            print(f"    âŒ ì‹¤íŒ¨: {e}")


if __name__ == "__main__":
    main()

    # ë™ì  ì²´ì¸ ë””ë²„ê¹…ë„ í•„ìš”í•˜ë©´ ì£¼ì„ í•´ì œ
    # debug_dynamic_chain()
